{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr           = 0.1\n",
    "\n",
    "start_epoch  = 1\n",
    "num_epochs   = 200\n",
    "batch_size   = 96\n",
    "\n",
    "is_use_cuda = torch.cuda.is_available()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "best_acc    = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Data Preprocess\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(64, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48024579, 0.44807218, 0.39754775), (0.27698641, 0.26906449, 0.28208191))\n",
    "])\n",
    "\n",
    "transform_test  = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.48236438, 0.44950216, 0.39812628), (0.27701401, 0.26931673, 0.2829424))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(os.path.join('E:\\\\WorkPlace\\\\pytorch_tiny_imagenet\\\\data', 'train'), transform_train)\n",
    "test_dataset  = datasets.ImageFolder(os.path.join('E:\\\\WorkPlace\\\\pytorch_tiny_imagenet\\\\data', 'val'), transform_test)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "test_loader   = torch.utils.data.DataLoader(test_dataset, batch_size=80, num_workers=8, shuffle=False)\n",
    "class_name    = train_dataset.classes\n",
    "#print(class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.1813 -0.0679  0.0274 -0.3348  0.1392 -0.1380  0.1955 -0.2704  0.1824 -0.1123\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.0124  0.0228 -0.2162 -0.0095 -0.1695 -0.3106  0.0397 -0.0540 -0.0636 -0.0091\n",
      "\n",
      "Columns 20 to 29 \n",
      "-0.2761  0.0735 -0.0585 -0.0479  0.0324  0.2498  0.0870  0.0452  0.2367 -0.0895\n",
      "\n",
      "Columns 30 to 39 \n",
      "-0.0943 -0.0118  0.2668  0.1365  0.0023 -0.0902 -0.1897 -0.4248  0.2003 -0.1374\n",
      "\n",
      "Columns 40 to 49 \n",
      "-0.2398  0.6363 -0.1924 -0.0227 -0.3340  0.0776  0.2507 -0.1656 -0.0578 -0.0871\n",
      "\n",
      "Columns 50 to 59 \n",
      "-0.0788 -0.2369 -0.1225 -0.1602 -0.3462 -0.1089  0.2045 -0.2057  0.1805  0.0797\n",
      "\n",
      "Columns 60 to 69 \n",
      "-0.3822 -0.2182  0.1988  0.0181 -0.0246 -0.3010  0.4239  0.1257  0.0499  0.1942\n",
      "\n",
      "Columns 70 to 79 \n",
      " 0.0482 -0.0360  0.3120 -0.2579  0.1448  0.0282 -0.0545 -0.1435 -0.2621 -0.2400\n",
      "\n",
      "Columns 80 to 89 \n",
      " 0.1645 -0.1296 -0.0147  0.0152 -0.0167  0.1685 -0.2259  0.0058 -0.1662 -0.0057\n",
      "\n",
      "Columns 90 to 99 \n",
      " 0.0900 -0.0200  0.1237  0.2340 -0.1799  0.1032 -0.2695  0.0700  0.0798  0.1424\n",
      "\n",
      "Columns 100 to 109 \n",
      " 0.1774  0.0206  0.0253 -0.1239 -0.3120  0.0259 -0.2508 -0.1759 -0.3384 -0.2822\n",
      "\n",
      "Columns 110 to 119 \n",
      "-0.0339 -0.0188  0.0642  0.3588  0.2916  0.1003  0.0413  0.2353  0.0171  0.3101\n",
      "\n",
      "Columns 120 to 129 \n",
      "-0.0883 -0.0401 -0.1458 -0.0554  0.3066 -0.2044  0.0995  0.2183 -0.2025  0.3580\n",
      "\n",
      "Columns 130 to 139 \n",
      " 0.2015  0.2696 -0.0518  0.2248  0.2313 -0.2051 -0.1492 -0.3357  0.1524 -0.0903\n",
      "\n",
      "Columns 140 to 149 \n",
      " 0.0568  0.0347 -0.0423 -0.1160  0.1907 -0.0114 -0.0457 -0.1380 -0.0001 -0.1746\n",
      "\n",
      "Columns 150 to 159 \n",
      "-0.0666  0.1878 -0.0433  0.3649 -0.1984 -0.3346  0.1433  0.2555 -0.0376 -0.0493\n",
      "\n",
      "Columns 160 to 169 \n",
      " 0.1087  0.0803 -0.0812 -0.2447  0.1200  0.3383 -0.0589  0.1279 -0.3045  0.1021\n",
      "\n",
      "Columns 170 to 179 \n",
      " 0.0124 -0.0220 -0.1897 -0.0034  0.2453  0.2723 -0.1647  0.3515  0.0440  0.1571\n",
      "\n",
      "Columns 180 to 189 \n",
      " 0.1500  0.4070 -0.0094 -0.0744  0.0777  0.0069 -0.3751 -0.1985 -0.0440  0.0642\n",
      "\n",
      "Columns 190 to 199 \n",
      "-0.1067 -0.0918 -0.0159 -0.0417  0.0020  0.2444 -0.1338 -0.1025  0.0511 -0.2898\n",
      "[torch.FloatTensor of size 1x200]\n",
      " <class 'torch.autograd.variable.Variable'> torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "class ShuffleBlock(nn.Module):\n",
    "    def __init__(self, groups):\n",
    "        super(ShuffleBlock, self).__init__()\n",
    "        self.groups = groups\n",
    "        \n",
    "    def forward(self, input):\n",
    "        n, c, h, w = input.size()\n",
    "        G          = self.groups\n",
    "        output     = input.view(n, G, c // G, h, w).permute(0, 2, 1, 3, 4).contiguous().view(n, c, h, w)\n",
    "        return output\n",
    "\n",
    "class IGCVBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, L, M, stride):\n",
    "        super(IGCVBlock, self).__init__()\n",
    "        if out_channel != in_channel:\n",
    "            M_t = out_channel // L\n",
    "        else:\n",
    "            M_t = M\n",
    "        \n",
    "        self.conv1    = nn.Conv2d(in_channel, out_channel, 3, stride, 1, groups=L, bias=True)\n",
    "        self.bn1      = nn.BatchNorm2d(out_channel)\n",
    "        self.shuffle1 = ShuffleBlock(L)\n",
    "        self.conv2    = nn.Conv2d(out_channel, out_channel, 1, groups=M_t, bias=True)\n",
    "        self.bn2      = nn.BatchNorm2d(out_channel)\n",
    "        self.shuffle2 = ShuffleBlock(M_t)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if 1 != stride or in_channel != out_channel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                            nn.Conv2d(in_channel, out_channel, 1, stride, bias=True),\n",
    "                            nn.BatchNorm2d(out_channel)\n",
    "            )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        feat = F.relu(self.bn1(self.conv1(input)))\n",
    "        feat = self.shuffle1(feat)\n",
    "        feat = self.bn2(self.conv2(feat))\n",
    "        feat = self.shuffle2(feat)\n",
    "        feat += self.shortcut(input)\n",
    "        feat = F.relu(feat)\n",
    "        return feat\n",
    "\n",
    "class IGCV_V1(nn.Module):\n",
    "    def __init__(self, L, M, D, is_L_twice, num_classes):\n",
    "        super(IGCV_V1, self).__init__()\n",
    "        \n",
    "        assert (D - 2) % 3 == 0, 'D must be equival to 3B + 2'\n",
    "        B = (D - 2) // 3\n",
    "        \n",
    "        self.in_channel = L * M\n",
    "        self.conv1  = nn.Conv2d(3, self.in_channel, 3, 1, 1, bias=True)\n",
    "        self.bn1    = nn.BatchNorm2d(self.in_channel)\n",
    "        self.stage1 = self._make_layers(self.in_channel, L, M, B)\n",
    "        self.in_channel *= 2\n",
    "        if is_L_twice:\n",
    "            L *= 2\n",
    "        else:\n",
    "            M *= 2\n",
    "        self.stage2 = self._make_layers(self.in_channel, L, M, B)\n",
    "        self.in_channel *= 2\n",
    "        if is_L_twice:\n",
    "            L *= 2\n",
    "        else:\n",
    "            M *= 2\n",
    "        self.stage3 = self._make_layers(self.in_channel, L, M, B)\n",
    "        self.linear = nn.Linear(self.in_channel * 2, num_classes)\n",
    "        \n",
    "    def _make_layers(self, in_channel, L, M, B):\n",
    "        strides = [2] + [1] * (B - 1)\n",
    "        layers = []\n",
    "        out_channel = self.in_channel\n",
    "        for i, _stride in enumerate(strides):\n",
    "            if i == len(strides) - 1:\n",
    "                out_channel *= 2\n",
    "            layers.append(IGCVBlock(self.in_channel, out_channel, L, M, _stride))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        feat = F.relu(self.bn1(self.conv1(input)))\n",
    "        feat = self.stage1(feat)\n",
    "        feat = self.stage2(feat)\n",
    "        feat = self.stage3(feat)\n",
    "        feat = F.avg_pool2d(feat, 8)\n",
    "        feat = feat.view(feat.size(0), -1)\n",
    "        out  = self.linear(feat)\n",
    "        return out\n",
    "    \n",
    "def IGCV_L24M2(num_classes):\n",
    "    return IGCV_V1(24, 2, 20, False, num_classes)\n",
    "\n",
    "net = IGCV_L24M2(200)\n",
    "x   = Variable(torch.randn(1, 3, 64, 64))\n",
    "y   = net(x)\n",
    "print(y, type(y), y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def conv_init(m):\n",
    "    class_name = m.__class__.__name__\n",
    "    if class_name.find('Conv') != -1:\n",
    "        init.xavier_uniform(m.weight, gain=np.sqrt(2))\n",
    "        init.constant(m.bias, 0)\n",
    "    elif class_name.find('BatchNorm') != -1:\n",
    "        init.constant(m.weight, 1)\n",
    "        init.constant(m.bias, 0)\n",
    "        \n",
    "net = IGCV_L24M2(200)\n",
    "net.apply(conv_init)\n",
    "if is_use_cuda:\n",
    "    net.cuda()\n",
    "    net = nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def lr_schedule(lr, epoch):\n",
    "    optim_factor = 0\n",
    "    if epoch > 160:\n",
    "        optim_factor = 3\n",
    "    elif epoch > 120:\n",
    "        optim_factor = 2\n",
    "    elif epoch > 60:\n",
    "        optim_factor = 1\n",
    "        \n",
    "    return lr * math.pow(0.2, optim_factor)\n",
    "\n",
    "def train(epoch):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct    = 0\n",
    "    total      = 0\n",
    "    optimizer  = optim.SGD(net.parameters(), lr=lr_schedule(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    print('Training Epoch: #%d, LR: %.4f'%(epoch, lr_schedule(lr, epoch)))\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "        if is_use_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs        = net(inputs)\n",
    "        loss           = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data[0]\n",
    "        _, predict = torch.max(outputs.data, 1)\n",
    "        total      += labels.size(0)\n",
    "        correct    += predict.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('[%s] Training Epoch [%d/%d] Iter[%d/%d]\\t\\tLoss: %.4f Acc@1: %.3f'\n",
    "                        % (time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())),\n",
    "                           epoch, num_epochs, idx, len(train_dataset) // batch_size, \n",
    "                          train_loss / (batch_size * (idx + 1)), correct / total))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct   = 0\n",
    "    total     = 0\n",
    "    for idx, (inputs, labels) in enumerate(test_loader):\n",
    "        if is_use_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs, volatile=True), Variable(labels)\n",
    "        outputs        = net(inputs)\n",
    "        loss           = criterion(outputs, labels)\n",
    "        \n",
    "        test_loss  += loss.data[0]\n",
    "        _, predict = torch.max(outputs.data, 1)\n",
    "        total      += labels.size(0)\n",
    "        correct    += predict.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('[%s] Testing Epoch [%d/%d] Iter[%d/%d]\\t\\tLoss: %.4f Acc@1: %.3f'\n",
    "                        % (time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())),\n",
    "                           epoch, num_epochs, idx, len(test_dataset) // 80, \n",
    "                          test_loss / (100 * (idx + 1)), correct / total))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    if correct / total > best_acc:\n",
    "        print()\n",
    "        print('Saving Model...')\n",
    "        state = {\n",
    "            'net': net.module if is_use_cuda else net,\n",
    "            'acc': correct / total,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        if not os.path.isdir('./checkpoint/IGCV_L24M2'):\n",
    "            os.makedirs('./checkpoint/IGCV_L24M2')\n",
    "        torch.save(state, './checkpoint/IGCV_L24M2/IGCV_L24M2_Tiny_ImageNet.ckpt')\n",
    "        best_acc = correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: #1, LR: 0.1000\n",
      "[2018-06-12 22:01:27] Training Epoch [1/200] Iter[1041/1041]\t\tLoss: 0.0471 Acc@1: 0.069\n",
      "[2018-06-12 22:01:42] Testing Epoch [1/200] Iter[124/125]\t\tLoss: 0.0414 Acc@1: 0.112\n",
      "Saving Model...\n",
      "\n",
      "Epoch #1 Cost 254s\n",
      "Training Epoch: #2, LR: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:159: UserWarning: Couldn't retrieve source code for container of type IGCV_V1. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "E:\\ProgramData\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:159: UserWarning: Couldn't retrieve source code for container of type IGCVBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "E:\\ProgramData\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:159: UserWarning: Couldn't retrieve source code for container of type ShuffleBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-12 22:05:32] Training Epoch [2/200] Iter[1041/1041]\t\tLoss: 0.0380 Acc@1: 0.178\n",
      "[2018-06-12 22:05:42] Testing Epoch [2/200] Iter[124/125]\t\tLoss: 0.0350 Acc@1: 0.211\n",
      "Saving Model...\n",
      "\n",
      "Epoch #2 Cost 240s\n",
      "Training Epoch: #3, LR: 0.1000\n",
      "[2018-06-12 22:09:32] Training Epoch [3/200] Iter[1041/1041]\t\tLoss: 0.0346 Acc@1: 0.234\n",
      "[2018-06-12 22:09:42] Testing Epoch [3/200] Iter[124/125]\t\tLoss: 0.0361 Acc@1: 0.195\n",
      "Epoch #3 Cost 239s\n",
      "Training Epoch: #4, LR: 0.1000\n",
      "[2018-06-12 22:13:32] Training Epoch [4/200] Iter[1041/1041]\t\tLoss: 0.0329 Acc@1: 0.264\n",
      "[2018-06-12 22:13:42] Testing Epoch [4/200] Iter[124/125]\t\tLoss: 0.0346 Acc@1: 0.237\n",
      "Saving Model...\n",
      "\n",
      "Epoch #4 Cost 240s\n",
      "Training Epoch: #5, LR: 0.1000\n",
      "[2018-06-12 22:17:33] Training Epoch [5/200] Iter[1041/1041]\t\tLoss: 0.0318 Acc@1: 0.285\n",
      "[2018-06-12 22:17:43] Testing Epoch [5/200] Iter[124/125]\t\tLoss: 0.0354 Acc@1: 0.244\n",
      "Saving Model...\n",
      "\n",
      "Epoch #5 Cost 240s\n",
      "Training Epoch: #6, LR: 0.1000\n",
      "[2018-06-12 22:21:33] Training Epoch [6/200] Iter[1041/1041]\t\tLoss: 0.0309 Acc@1: 0.302\n",
      "[2018-06-12 22:21:43] Testing Epoch [6/200] Iter[124/125]\t\tLoss: 0.0326 Acc@1: 0.253\n",
      "Saving Model...\n",
      "\n",
      "Epoch #6 Cost 240s\n",
      "Training Epoch: #7, LR: 0.1000\n",
      "[2018-06-12 22:25:33] Training Epoch [7/200] Iter[1041/1041]\t\tLoss: 0.0304 Acc@1: 0.312\n",
      "[2018-06-12 22:25:43] Testing Epoch [7/200] Iter[124/125]\t\tLoss: 0.0368 Acc@1: 0.190\n",
      "Epoch #7 Cost 240s\n",
      "Training Epoch: #8, LR: 0.1000\n",
      "[2018-06-12 22:29:32] Training Epoch [8/200] Iter[1041/1041]\t\tLoss: 0.0299 Acc@1: 0.322\n",
      "[2018-06-12 22:29:42] Testing Epoch [8/200] Iter[124/125]\t\tLoss: 0.0310 Acc@1: 0.286\n",
      "Saving Model...\n",
      "\n",
      "Epoch #8 Cost 239s\n",
      "Training Epoch: #9, LR: 0.1000\n",
      "[2018-06-12 22:33:33] Training Epoch [9/200] Iter[1041/1041]\t\tLoss: 0.0295 Acc@1: 0.331\n",
      "[2018-06-12 22:33:43] Testing Epoch [9/200] Iter[124/125]\t\tLoss: 0.0308 Acc@1: 0.286\n",
      "Epoch #9 Cost 240s\n",
      "Training Epoch: #10, LR: 0.1000\n",
      "[2018-06-12 22:37:33] Training Epoch [10/200] Iter[1041/1041]\t\tLoss: 0.0291 Acc@1: 0.338\n",
      "[2018-06-12 22:37:43] Testing Epoch [10/200] Iter[124/125]\t\tLoss: 0.0334 Acc@1: 0.253\n",
      "Epoch #10 Cost 240s\n",
      "Training Epoch: #11, LR: 0.1000\n",
      "[2018-06-12 22:41:32] Training Epoch [11/200] Iter[1041/1041]\t\tLoss: 0.0288 Acc@1: 0.343\n",
      "[2018-06-12 22:41:42] Testing Epoch [11/200] Iter[124/125]\t\tLoss: 0.0310 Acc@1: 0.298\n",
      "Saving Model...\n",
      "\n",
      "Epoch #11 Cost 238s\n",
      "Training Epoch: #12, LR: 0.1000\n",
      "[2018-06-12 22:45:31] Training Epoch [12/200] Iter[1041/1041]\t\tLoss: 0.0286 Acc@1: 0.348\n",
      "[2018-06-12 22:45:41] Testing Epoch [12/200] Iter[124/125]\t\tLoss: 0.0313 Acc@1: 0.288\n",
      "Epoch #12 Cost 239s\n",
      "Training Epoch: #13, LR: 0.1000\n",
      "[2018-06-12 22:49:31] Training Epoch [13/200] Iter[1041/1041]\t\tLoss: 0.0283 Acc@1: 0.350\n",
      "[2018-06-12 22:49:41] Testing Epoch [13/200] Iter[124/125]\t\tLoss: 0.0311 Acc@1: 0.292\n",
      "Epoch #13 Cost 239s\n",
      "Training Epoch: #14, LR: 0.1000\n",
      "[2018-06-12 22:53:31] Training Epoch [14/200] Iter[1041/1041]\t\tLoss: 0.0281 Acc@1: 0.356\n",
      "[2018-06-12 22:53:41] Testing Epoch [14/200] Iter[124/125]\t\tLoss: 0.0306 Acc@1: 0.299\n",
      "Saving Model...\n",
      "\n",
      "Epoch #14 Cost 239s\n",
      "Training Epoch: #15, LR: 0.1000\n",
      "[2018-06-12 22:57:31] Training Epoch [15/200] Iter[1041/1041]\t\tLoss: 0.0280 Acc@1: 0.357\n",
      "[2018-06-12 22:57:41] Testing Epoch [15/200] Iter[124/125]\t\tLoss: 0.0307 Acc@1: 0.301\n",
      "Saving Model...\n",
      "\n",
      "Epoch #15 Cost 240s\n",
      "Training Epoch: #16, LR: 0.1000\n",
      "[2018-06-12 23:01:29] Training Epoch [16/200] Iter[1041/1041]\t\tLoss: 0.0279 Acc@1: 0.359\n",
      "[2018-06-12 23:01:39] Testing Epoch [16/200] Iter[124/125]\t\tLoss: 0.0332 Acc@1: 0.270\n",
      "Epoch #16 Cost 238s\n",
      "Training Epoch: #17, LR: 0.1000\n",
      "[2018-06-12 23:05:29] Training Epoch [17/200] Iter[1041/1041]\t\tLoss: 0.0277 Acc@1: 0.363\n",
      "[2018-06-12 23:05:39] Testing Epoch [17/200] Iter[124/125]\t\tLoss: 0.0333 Acc@1: 0.275\n",
      "Epoch #17 Cost 239s\n",
      "Training Epoch: #18, LR: 0.1000\n",
      "[2018-06-12 23:09:29] Training Epoch [18/200] Iter[1041/1041]\t\tLoss: 0.0276 Acc@1: 0.369\n",
      "[2018-06-12 23:09:39] Testing Epoch [18/200] Iter[124/125]\t\tLoss: 0.0326 Acc@1: 0.270\n",
      "Epoch #18 Cost 240s\n",
      "Training Epoch: #19, LR: 0.1000\n",
      "[2018-06-12 23:13:27] Training Epoch [19/200] Iter[1041/1041]\t\tLoss: 0.0275 Acc@1: 0.368\n",
      "[2018-06-12 23:13:37] Testing Epoch [19/200] Iter[124/125]\t\tLoss: 0.0302 Acc@1: 0.300\n",
      "Epoch #19 Cost 238s\n",
      "Training Epoch: #20, LR: 0.1000\n",
      "[2018-06-12 23:17:27] Training Epoch [20/200] Iter[1041/1041]\t\tLoss: 0.0274 Acc@1: 0.371\n",
      "[2018-06-12 23:17:37] Testing Epoch [20/200] Iter[124/125]\t\tLoss: 0.0321 Acc@1: 0.277\n",
      "Epoch #20 Cost 239s\n",
      "Training Epoch: #21, LR: 0.1000\n",
      "[2018-06-12 23:21:27] Training Epoch [21/200] Iter[1041/1041]\t\tLoss: 0.0274 Acc@1: 0.371\n",
      "[2018-06-12 23:21:37] Testing Epoch [21/200] Iter[124/125]\t\tLoss: 0.0306 Acc@1: 0.305\n",
      "Saving Model...\n",
      "\n",
      "Epoch #21 Cost 239s\n",
      "Training Epoch: #22, LR: 0.1000\n",
      "[2018-06-12 23:25:27] Training Epoch [22/200] Iter[1041/1041]\t\tLoss: 0.0273 Acc@1: 0.372\n",
      "[2018-06-12 23:25:37] Testing Epoch [22/200] Iter[124/125]\t\tLoss: 0.0298 Acc@1: 0.314\n",
      "Saving Model...\n",
      "\n",
      "Epoch #22 Cost 239s\n",
      "Training Epoch: #23, LR: 0.1000\n",
      "[2018-06-12 23:29:26] Training Epoch [23/200] Iter[1041/1041]\t\tLoss: 0.0272 Acc@1: 0.373\n",
      "[2018-06-12 23:29:36] Testing Epoch [23/200] Iter[124/125]\t\tLoss: 0.0342 Acc@1: 0.274\n",
      "Epoch #23 Cost 239s\n",
      "Training Epoch: #24, LR: 0.1000\n",
      "[2018-06-12 23:33:26] Training Epoch [24/200] Iter[1041/1041]\t\tLoss: 0.0271 Acc@1: 0.376\n",
      "[2018-06-12 23:33:36] Testing Epoch [24/200] Iter[124/125]\t\tLoss: 0.0285 Acc@1: 0.335\n",
      "Saving Model...\n",
      "\n",
      "Epoch #24 Cost 239s\n",
      "Training Epoch: #25, LR: 0.1000\n",
      "[2018-06-12 23:37:26] Training Epoch [25/200] Iter[1041/1041]\t\tLoss: 0.0271 Acc@1: 0.377\n",
      "[2018-06-12 23:37:36] Testing Epoch [25/200] Iter[124/125]\t\tLoss: 0.0284 Acc@1: 0.338\n",
      "Saving Model...\n",
      "\n",
      "Epoch #25 Cost 240s\n",
      "Training Epoch: #26, LR: 0.1000\n",
      "[2018-06-12 23:41:26] Training Epoch [26/200] Iter[1041/1041]\t\tLoss: 0.0270 Acc@1: 0.380\n",
      "[2018-06-12 23:41:36] Testing Epoch [26/200] Iter[124/125]\t\tLoss: 0.0312 Acc@1: 0.290\n",
      "Epoch #26 Cost 239s\n",
      "Training Epoch: #27, LR: 0.1000\n",
      "[2018-06-12 23:45:26] Training Epoch [27/200] Iter[1041/1041]\t\tLoss: 0.0270 Acc@1: 0.380\n",
      "[2018-06-12 23:45:36] Testing Epoch [27/200] Iter[124/125]\t\tLoss: 0.0293 Acc@1: 0.315\n",
      "Epoch #27 Cost 240s\n",
      "Training Epoch: #28, LR: 0.1000\n",
      "[2018-06-12 23:49:27] Training Epoch [28/200] Iter[1041/1041]\t\tLoss: 0.0269 Acc@1: 0.382\n",
      "[2018-06-12 23:49:37] Testing Epoch [28/200] Iter[124/125]\t\tLoss: 0.0289 Acc@1: 0.323\n",
      "Epoch #28 Cost 240s\n",
      "Training Epoch: #29, LR: 0.1000\n",
      "[2018-06-12 23:53:26] Training Epoch [29/200] Iter[1041/1041]\t\tLoss: 0.0269 Acc@1: 0.380\n",
      "[2018-06-12 23:53:37] Testing Epoch [29/200] Iter[124/125]\t\tLoss: 0.0297 Acc@1: 0.310\n",
      "Epoch #29 Cost 239s\n",
      "Training Epoch: #30, LR: 0.1000\n",
      "[2018-06-12 23:57:27] Training Epoch [30/200] Iter[1041/1041]\t\tLoss: 0.0268 Acc@1: 0.383\n",
      "[2018-06-12 23:57:37] Testing Epoch [30/200] Iter[124/125]\t\tLoss: 0.0328 Acc@1: 0.266\n",
      "Epoch #30 Cost 240s\n",
      "Training Epoch: #31, LR: 0.1000\n",
      "[2018-06-13 00:01:26] Training Epoch [31/200] Iter[1041/1041]\t\tLoss: 0.0268 Acc@1: 0.383\n",
      "[2018-06-13 00:01:36] Testing Epoch [31/200] Iter[124/125]\t\tLoss: 0.0307 Acc@1: 0.293\n",
      "Epoch #31 Cost 239s\n",
      "Training Epoch: #32, LR: 0.1000\n",
      "[2018-06-13 00:05:25] Training Epoch [32/200] Iter[1041/1041]\t\tLoss: 0.0268 Acc@1: 0.383\n",
      "[2018-06-13 00:05:36] Testing Epoch [32/200] Iter[124/125]\t\tLoss: 0.0305 Acc@1: 0.303\n",
      "Epoch #32 Cost 239s\n",
      "Training Epoch: #33, LR: 0.1000\n",
      "[2018-06-13 00:09:25] Training Epoch [33/200] Iter[1041/1041]\t\tLoss: 0.0267 Acc@1: 0.384\n",
      "[2018-06-13 00:09:35] Testing Epoch [33/200] Iter[124/125]\t\tLoss: 0.0310 Acc@1: 0.297\n",
      "Epoch #33 Cost 239s\n",
      "Training Epoch: #34, LR: 0.1000\n",
      "[2018-06-13 00:13:24] Training Epoch [34/200] Iter[1041/1041]\t\tLoss: 0.0268 Acc@1: 0.383\n",
      "[2018-06-13 00:13:35] Testing Epoch [34/200] Iter[124/125]\t\tLoss: 0.0303 Acc@1: 0.310\n",
      "Epoch #34 Cost 239s\n",
      "Training Epoch: #35, LR: 0.1000\n",
      "[2018-06-13 00:17:23] Training Epoch [35/200] Iter[1041/1041]\t\tLoss: 0.0267 Acc@1: 0.385\n",
      "[2018-06-13 00:17:33] Testing Epoch [35/200] Iter[124/125]\t\tLoss: 0.0300 Acc@1: 0.314\n",
      "Epoch #35 Cost 238s\n",
      "Training Epoch: #36, LR: 0.1000\n",
      "[2018-06-13 00:21:22] Training Epoch [36/200] Iter[1041/1041]\t\tLoss: 0.0266 Acc@1: 0.388\n",
      "[2018-06-13 00:21:32] Testing Epoch [36/200] Iter[124/125]\t\tLoss: 0.0305 Acc@1: 0.308\n",
      "Epoch #36 Cost 239s\n",
      "Training Epoch: #37, LR: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-13 00:25:22] Training Epoch [37/200] Iter[1041/1041]\t\tLoss: 0.0266 Acc@1: 0.386\n",
      "[2018-06-13 00:25:32] Testing Epoch [37/200] Iter[124/125]\t\tLoss: 0.0286 Acc@1: 0.329\n",
      "Epoch #37 Cost 240s\n",
      "Training Epoch: #38, LR: 0.1000\n",
      "[2018-06-13 00:29:22] Training Epoch [38/200] Iter[1041/1041]\t\tLoss: 0.0266 Acc@1: 0.387\n",
      "[2018-06-13 00:29:32] Testing Epoch [38/200] Iter[124/125]\t\tLoss: 0.0302 Acc@1: 0.300\n",
      "Epoch #38 Cost 239s\n",
      "Training Epoch: #39, LR: 0.1000\n",
      "[2018-06-13 00:33:22] Training Epoch [39/200] Iter[1041/1041]\t\tLoss: 0.0266 Acc@1: 0.388\n",
      "[2018-06-13 00:33:32] Testing Epoch [39/200] Iter[124/125]\t\tLoss: 0.0316 Acc@1: 0.284\n",
      "Epoch #39 Cost 239s\n",
      "Training Epoch: #40, LR: 0.1000\n",
      "[2018-06-13 00:37:21] Training Epoch [40/200] Iter[1041/1041]\t\tLoss: 0.0266 Acc@1: 0.388\n",
      "[2018-06-13 00:37:32] Testing Epoch [40/200] Iter[124/125]\t\tLoss: 0.0296 Acc@1: 0.326\n",
      "Epoch #40 Cost 239s\n",
      "Training Epoch: #41, LR: 0.1000\n",
      "[2018-06-13 00:41:20] Training Epoch [41/200] Iter[1041/1041]\t\tLoss: 0.0266 Acc@1: 0.388\n",
      "[2018-06-13 00:41:30] Testing Epoch [41/200] Iter[124/125]\t\tLoss: 0.0359 Acc@1: 0.243\n",
      "Epoch #41 Cost 238s\n",
      "Training Epoch: #42, LR: 0.1000\n",
      "[2018-06-13 00:45:19] Training Epoch [42/200] Iter[1041/1041]\t\tLoss: 0.0266 Acc@1: 0.387\n",
      "[2018-06-13 00:45:29] Testing Epoch [42/200] Iter[124/125]\t\tLoss: 0.0290 Acc@1: 0.331\n",
      "Epoch #42 Cost 239s\n",
      "Training Epoch: #43, LR: 0.1000\n",
      "[2018-06-13 00:49:19] Training Epoch [43/200] Iter[1041/1041]\t\tLoss: 0.0265 Acc@1: 0.390\n",
      "[2018-06-13 00:49:29] Testing Epoch [43/200] Iter[124/125]\t\tLoss: 0.0282 Acc@1: 0.339\n",
      "Saving Model...\n",
      "\n",
      "Epoch #43 Cost 239s\n",
      "Training Epoch: #44, LR: 0.1000\n",
      "[2018-06-13 00:53:19] Training Epoch [44/200] Iter[1041/1041]\t\tLoss: 0.0265 Acc@1: 0.390\n",
      "[2018-06-13 00:53:29] Testing Epoch [44/200] Iter[124/125]\t\tLoss: 0.0319 Acc@1: 0.287\n",
      "Epoch #44 Cost 239s\n",
      "Training Epoch: #45, LR: 0.1000\n",
      "[2018-06-13 00:57:18] Training Epoch [45/200] Iter[1041/1041]\t\tLoss: 0.0265 Acc@1: 0.389\n",
      "[2018-06-13 00:57:28] Testing Epoch [45/200] Iter[124/125]\t\tLoss: 0.0309 Acc@1: 0.302\n",
      "Epoch #45 Cost 239s\n",
      "Training Epoch: #46, LR: 0.1000\n",
      "[2018-06-13 01:01:18] Training Epoch [46/200] Iter[1041/1041]\t\tLoss: 0.0265 Acc@1: 0.389\n",
      "[2018-06-13 01:01:28] Testing Epoch [46/200] Iter[124/125]\t\tLoss: 0.0345 Acc@1: 0.258\n",
      "Epoch #46 Cost 239s\n",
      "Training Epoch: #47, LR: 0.1000\n",
      "[2018-06-13 01:05:19] Training Epoch [47/200] Iter[1041/1041]\t\tLoss: 0.0265 Acc@1: 0.390\n",
      "[2018-06-13 01:05:30] Testing Epoch [47/200] Iter[124/125]\t\tLoss: 0.0312 Acc@1: 0.306\n",
      "Epoch #47 Cost 241s\n",
      "Training Epoch: #48, LR: 0.1000\n",
      "[2018-06-13 01:09:20] Training Epoch [48/200] Iter[1041/1041]\t\tLoss: 0.0264 Acc@1: 0.391\n",
      "[2018-06-13 01:09:30] Testing Epoch [48/200] Iter[124/125]\t\tLoss: 0.0322 Acc@1: 0.282\n",
      "Epoch #48 Cost 240s\n",
      "Training Epoch: #49, LR: 0.1000\n",
      "[2018-06-13 01:13:21] Training Epoch [49/200] Iter[1041/1041]\t\tLoss: 0.0264 Acc@1: 0.390\n",
      "[2018-06-13 01:13:31] Testing Epoch [49/200] Iter[124/125]\t\tLoss: 0.0298 Acc@1: 0.311\n",
      "Epoch #49 Cost 241s\n",
      "Training Epoch: #50, LR: 0.1000\n",
      "[2018-06-13 01:17:20] Training Epoch [50/200] Iter[1041/1041]\t\tLoss: 0.0264 Acc@1: 0.392\n",
      "[2018-06-13 01:17:30] Testing Epoch [50/200] Iter[124/125]\t\tLoss: 0.0288 Acc@1: 0.328\n",
      "Epoch #50 Cost 238s\n",
      "Training Epoch: #51, LR: 0.1000\n",
      "[2018-06-13 01:21:21] Training Epoch [51/200] Iter[1041/1041]\t\tLoss: 0.0264 Acc@1: 0.390\n",
      "[2018-06-13 01:21:31] Testing Epoch [51/200] Iter[124/125]\t\tLoss: 0.0289 Acc@1: 0.334\n",
      "Epoch #51 Cost 241s\n",
      "Training Epoch: #52, LR: 0.1000\n",
      "[2018-06-13 01:25:22] Training Epoch [52/200] Iter[1041/1041]\t\tLoss: 0.0264 Acc@1: 0.391\n",
      "[2018-06-13 01:25:32] Testing Epoch [52/200] Iter[124/125]\t\tLoss: 0.0289 Acc@1: 0.327\n",
      "Epoch #52 Cost 240s\n",
      "Training Epoch: #53, LR: 0.1000\n",
      "[2018-06-13 01:29:22] Training Epoch [53/200] Iter[1041/1041]\t\tLoss: 0.0265 Acc@1: 0.389\n",
      "[2018-06-13 01:29:34] Testing Epoch [53/200] Iter[124/125]\t\tLoss: 0.0299 Acc@1: 0.316\n",
      "Epoch #53 Cost 242s\n",
      "Training Epoch: #54, LR: 0.1000\n",
      "[2018-06-13 01:33:26] Training Epoch [54/200] Iter[1041/1041]\t\tLoss: 0.0264 Acc@1: 0.391\n",
      "[2018-06-13 01:33:37] Testing Epoch [54/200] Iter[124/125]\t\tLoss: 0.0287 Acc@1: 0.333\n",
      "Epoch #54 Cost 242s\n",
      "Training Epoch: #55, LR: 0.1000\n",
      "[2018-06-13 01:37:24] Training Epoch [55/200] Iter[1041/1041]\t\tLoss: 0.0264 Acc@1: 0.394\n",
      "[2018-06-13 01:37:34] Testing Epoch [55/200] Iter[124/125]\t\tLoss: 0.0300 Acc@1: 0.324\n",
      "Epoch #55 Cost 237s\n",
      "Training Epoch: #56, LR: 0.1000\n",
      "[2018-06-13 01:41:23] Training Epoch [56/200] Iter[1041/1041]\t\tLoss: 0.0264 Acc@1: 0.393\n",
      "[2018-06-13 01:41:33] Testing Epoch [56/200] Iter[124/125]\t\tLoss: 0.0301 Acc@1: 0.305\n",
      "Epoch #56 Cost 238s\n",
      "Training Epoch: #57, LR: 0.1000\n",
      "[2018-06-13 01:45:22] Training Epoch [57/200] Iter[1041/1041]\t\tLoss: 0.0264 Acc@1: 0.390\n",
      "[2018-06-13 01:45:32] Testing Epoch [57/200] Iter[124/125]\t\tLoss: 0.0303 Acc@1: 0.305\n",
      "Epoch #57 Cost 238s\n",
      "Training Epoch: #58, LR: 0.1000\n",
      "[2018-06-13 01:49:21] Training Epoch [58/200] Iter[1041/1041]\t\tLoss: 0.0264 Acc@1: 0.392\n",
      "[2018-06-13 01:49:31] Testing Epoch [58/200] Iter[124/125]\t\tLoss: 0.0296 Acc@1: 0.329\n",
      "Epoch #58 Cost 239s\n",
      "Training Epoch: #59, LR: 0.1000\n",
      "[2018-06-13 01:53:21] Training Epoch [59/200] Iter[1041/1041]\t\tLoss: 0.0264 Acc@1: 0.393\n",
      "[2018-06-13 01:53:31] Testing Epoch [59/200] Iter[124/125]\t\tLoss: 0.0313 Acc@1: 0.293\n",
      "Epoch #59 Cost 239s\n",
      "Training Epoch: #60, LR: 0.1000\n",
      "[2018-06-13 01:57:20] Training Epoch [60/200] Iter[1041/1041]\t\tLoss: 0.0263 Acc@1: 0.394\n",
      "[2018-06-13 01:57:30] Testing Epoch [60/200] Iter[124/125]\t\tLoss: 0.0359 Acc@1: 0.247\n",
      "Epoch #60 Cost 238s\n",
      "Training Epoch: #61, LR: 0.0200\n",
      "[2018-06-13 02:01:20] Training Epoch [61/200] Iter[1041/1041]\t\tLoss: 0.0211 Acc@1: 0.502\n",
      "[2018-06-13 02:01:30] Testing Epoch [61/200] Iter[124/125]\t\tLoss: 0.0205 Acc@1: 0.499\n",
      "Saving Model...\n",
      "\n",
      "Epoch #61 Cost 240s\n",
      "Training Epoch: #62, LR: 0.0200\n",
      "[2018-06-13 02:05:20] Training Epoch [62/200] Iter[1041/1041]\t\tLoss: 0.0200 Acc@1: 0.527\n",
      "[2018-06-13 02:05:30] Testing Epoch [62/200] Iter[124/125]\t\tLoss: 0.0204 Acc@1: 0.501\n",
      "Saving Model...\n",
      "\n",
      "Epoch #62 Cost 239s\n",
      "Training Epoch: #63, LR: 0.0200\n",
      "[2018-06-13 02:09:21] Training Epoch [63/200] Iter[1041/1041]\t\tLoss: 0.0198 Acc@1: 0.527\n",
      "[2018-06-13 02:09:31] Testing Epoch [63/200] Iter[124/125]\t\tLoss: 0.0207 Acc@1: 0.494\n",
      "Epoch #63 Cost 241s\n",
      "Training Epoch: #64, LR: 0.0200\n",
      "[2018-06-13 02:13:21] Training Epoch [64/200] Iter[1041/1041]\t\tLoss: 0.0197 Acc@1: 0.531\n",
      "[2018-06-13 02:13:31] Testing Epoch [64/200] Iter[124/125]\t\tLoss: 0.0214 Acc@1: 0.489\n",
      "Epoch #64 Cost 239s\n",
      "Training Epoch: #65, LR: 0.0200\n",
      "[2018-06-13 02:17:21] Training Epoch [65/200] Iter[1041/1041]\t\tLoss: 0.0197 Acc@1: 0.529\n",
      "[2018-06-13 02:17:32] Testing Epoch [65/200] Iter[124/125]\t\tLoss: 0.0213 Acc@1: 0.483\n",
      "Epoch #65 Cost 240s\n",
      "Training Epoch: #66, LR: 0.0200\n",
      "[2018-06-13 02:21:21] Training Epoch [66/200] Iter[1041/1041]\t\tLoss: 0.0198 Acc@1: 0.527\n",
      "[2018-06-13 02:21:31] Testing Epoch [66/200] Iter[124/125]\t\tLoss: 0.0213 Acc@1: 0.479\n",
      "Epoch #66 Cost 239s\n",
      "Training Epoch: #67, LR: 0.0200\n",
      "[2018-06-13 02:25:22] Training Epoch [67/200] Iter[1041/1041]\t\tLoss: 0.0198 Acc@1: 0.527\n",
      "[2018-06-13 02:25:32] Testing Epoch [67/200] Iter[124/125]\t\tLoss: 0.0211 Acc@1: 0.482\n",
      "Epoch #67 Cost 241s\n",
      "Training Epoch: #68, LR: 0.0200\n",
      "[2018-06-13 02:29:21] Training Epoch [68/200] Iter[1041/1041]\t\tLoss: 0.0197 Acc@1: 0.527\n",
      "[2018-06-13 02:29:32] Testing Epoch [68/200] Iter[124/125]\t\tLoss: 0.0212 Acc@1: 0.482\n",
      "Epoch #68 Cost 239s\n",
      "Training Epoch: #69, LR: 0.0200\n",
      "[2018-06-13 02:33:22] Training Epoch [69/200] Iter[1041/1041]\t\tLoss: 0.0198 Acc@1: 0.525\n",
      "[2018-06-13 02:33:32] Testing Epoch [69/200] Iter[124/125]\t\tLoss: 0.0210 Acc@1: 0.488\n",
      "Epoch #69 Cost 240s\n",
      "Training Epoch: #70, LR: 0.0200\n",
      "[2018-06-13 02:37:21] Training Epoch [70/200] Iter[1041/1041]\t\tLoss: 0.0197 Acc@1: 0.528\n",
      "[2018-06-13 02:37:31] Testing Epoch [70/200] Iter[124/125]\t\tLoss: 0.0218 Acc@1: 0.475\n",
      "Epoch #70 Cost 239s\n",
      "Training Epoch: #71, LR: 0.0200\n",
      "[2018-06-13 02:41:22] Training Epoch [71/200] Iter[1041/1041]\t\tLoss: 0.0197 Acc@1: 0.529\n",
      "[2018-06-13 02:41:32] Testing Epoch [71/200] Iter[124/125]\t\tLoss: 0.0218 Acc@1: 0.472\n",
      "Epoch #71 Cost 240s\n",
      "Training Epoch: #72, LR: 0.0200\n",
      "[2018-06-13 02:45:22] Training Epoch [72/200] Iter[1041/1041]\t\tLoss: 0.0196 Acc@1: 0.530\n",
      "[2018-06-13 02:45:32] Testing Epoch [72/200] Iter[124/125]\t\tLoss: 0.0214 Acc@1: 0.480\n",
      "Epoch #72 Cost 240s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: #73, LR: 0.0200\n",
      "[2018-06-13 02:49:23] Training Epoch [73/200] Iter[1041/1041]\t\tLoss: 0.0197 Acc@1: 0.528\n",
      "[2018-06-13 02:49:33] Testing Epoch [73/200] Iter[124/125]\t\tLoss: 0.0222 Acc@1: 0.469\n",
      "Epoch #73 Cost 240s\n",
      "Training Epoch: #74, LR: 0.0200\n",
      "[2018-06-13 02:53:24] Training Epoch [74/200] Iter[1041/1041]\t\tLoss: 0.0195 Acc@1: 0.532\n",
      "[2018-06-13 02:53:34] Testing Epoch [74/200] Iter[124/125]\t\tLoss: 0.0215 Acc@1: 0.480\n",
      "Epoch #74 Cost 240s\n",
      "Training Epoch: #75, LR: 0.0200\n",
      "[2018-06-13 02:57:24] Training Epoch [75/200] Iter[1041/1041]\t\tLoss: 0.0195 Acc@1: 0.532\n",
      "[2018-06-13 02:57:34] Testing Epoch [75/200] Iter[124/125]\t\tLoss: 0.0217 Acc@1: 0.475\n",
      "Epoch #75 Cost 240s\n",
      "Training Epoch: #76, LR: 0.0200\n",
      "[2018-06-13 03:01:24] Training Epoch [76/200] Iter[1041/1041]\t\tLoss: 0.0195 Acc@1: 0.531\n",
      "[2018-06-13 03:01:34] Testing Epoch [76/200] Iter[124/125]\t\tLoss: 0.0225 Acc@1: 0.469\n",
      "Epoch #76 Cost 239s\n",
      "Training Epoch: #77, LR: 0.0200\n",
      "[2018-06-13 03:05:25] Training Epoch [77/200] Iter[1041/1041]\t\tLoss: 0.0194 Acc@1: 0.533\n",
      "[2018-06-13 03:05:35] Testing Epoch [77/200] Iter[124/125]\t\tLoss: 0.0214 Acc@1: 0.481\n",
      "Epoch #77 Cost 240s\n",
      "Training Epoch: #78, LR: 0.0200\n",
      "[2018-06-13 03:09:24] Training Epoch [78/200] Iter[1041/1041]\t\tLoss: 0.0194 Acc@1: 0.532\n",
      "[2018-06-13 03:09:35] Testing Epoch [78/200] Iter[124/125]\t\tLoss: 0.0215 Acc@1: 0.476\n",
      "Epoch #78 Cost 239s\n",
      "Training Epoch: #79, LR: 0.0200\n",
      "[2018-06-13 03:13:23] Training Epoch [79/200] Iter[1041/1041]\t\tLoss: 0.0194 Acc@1: 0.533\n",
      "[2018-06-13 03:13:33] Testing Epoch [79/200] Iter[124/125]\t\tLoss: 0.0219 Acc@1: 0.472\n",
      "Epoch #79 Cost 238s\n",
      "Training Epoch: #80, LR: 0.0200\n",
      "[2018-06-13 03:17:24] Training Epoch [80/200] Iter[1041/1041]\t\tLoss: 0.0193 Acc@1: 0.535\n",
      "[2018-06-13 03:17:34] Testing Epoch [80/200] Iter[124/125]\t\tLoss: 0.0208 Acc@1: 0.494\n",
      "Epoch #80 Cost 240s\n",
      "Training Epoch: #81, LR: 0.0200\n",
      "[2018-06-13 03:21:24] Training Epoch [81/200] Iter[1041/1041]\t\tLoss: 0.0194 Acc@1: 0.534\n",
      "[2018-06-13 03:21:34] Testing Epoch [81/200] Iter[124/125]\t\tLoss: 0.0221 Acc@1: 0.464\n",
      "Epoch #81 Cost 239s\n",
      "Training Epoch: #82, LR: 0.0200\n",
      "[2018-06-13 03:25:23] Training Epoch [82/200] Iter[1041/1041]\t\tLoss: 0.0193 Acc@1: 0.535\n",
      "[2018-06-13 03:25:33] Testing Epoch [82/200] Iter[124/125]\t\tLoss: 0.0221 Acc@1: 0.472\n",
      "Epoch #82 Cost 238s\n",
      "Training Epoch: #83, LR: 0.0200\n",
      "[2018-06-13 03:29:23] Training Epoch [83/200] Iter[1041/1041]\t\tLoss: 0.0193 Acc@1: 0.536\n",
      "[2018-06-13 03:29:34] Testing Epoch [83/200] Iter[124/125]\t\tLoss: 0.0215 Acc@1: 0.476\n",
      "Epoch #83 Cost 240s\n",
      "Training Epoch: #84, LR: 0.0200\n",
      "[2018-06-13 03:33:24] Training Epoch [84/200] Iter[1041/1041]\t\tLoss: 0.0192 Acc@1: 0.539\n",
      "[2018-06-13 03:33:35] Testing Epoch [84/200] Iter[124/125]\t\tLoss: 0.0223 Acc@1: 0.463\n",
      "Epoch #84 Cost 241s\n",
      "Training Epoch: #85, LR: 0.0200\n",
      "[2018-06-13 03:37:23] Training Epoch [85/200] Iter[1041/1041]\t\tLoss: 0.0193 Acc@1: 0.537\n",
      "[2018-06-13 03:37:34] Testing Epoch [85/200] Iter[124/125]\t\tLoss: 0.0216 Acc@1: 0.484\n",
      "Epoch #85 Cost 238s\n",
      "Training Epoch: #86, LR: 0.0200\n",
      "[2018-06-13 03:41:23] Training Epoch [86/200] Iter[1041/1041]\t\tLoss: 0.0192 Acc@1: 0.537\n",
      "[2018-06-13 03:41:33] Testing Epoch [86/200] Iter[124/125]\t\tLoss: 0.0216 Acc@1: 0.474\n",
      "Epoch #86 Cost 239s\n",
      "Training Epoch: #87, LR: 0.0200\n",
      "[2018-06-13 03:45:24] Training Epoch [87/200] Iter[1041/1041]\t\tLoss: 0.0192 Acc@1: 0.539\n",
      "[2018-06-13 03:45:34] Testing Epoch [87/200] Iter[124/125]\t\tLoss: 0.0211 Acc@1: 0.484\n",
      "Epoch #87 Cost 241s\n",
      "Training Epoch: #88, LR: 0.0200\n",
      "[2018-06-13 03:49:24] Training Epoch [88/200] Iter[1041/1041]\t\tLoss: 0.0192 Acc@1: 0.538\n",
      "[2018-06-13 03:49:34] Testing Epoch [88/200] Iter[124/125]\t\tLoss: 0.0218 Acc@1: 0.468\n",
      "Epoch #88 Cost 240s\n",
      "Training Epoch: #89, LR: 0.0200\n",
      "[2018-06-13 03:53:26] Training Epoch [89/200] Iter[1041/1041]\t\tLoss: 0.0191 Acc@1: 0.538\n",
      "[2018-06-13 03:53:36] Testing Epoch [89/200] Iter[124/125]\t\tLoss: 0.0212 Acc@1: 0.490\n",
      "Epoch #89 Cost 241s\n",
      "Training Epoch: #90, LR: 0.0200\n",
      "[2018-06-13 03:57:25] Training Epoch [90/200] Iter[1041/1041]\t\tLoss: 0.0192 Acc@1: 0.539\n",
      "[2018-06-13 03:57:35] Testing Epoch [90/200] Iter[124/125]\t\tLoss: 0.0220 Acc@1: 0.473\n",
      "Epoch #90 Cost 239s\n",
      "Training Epoch: #91, LR: 0.0200\n",
      "[2018-06-13 04:01:26] Training Epoch [91/200] Iter[1041/1041]\t\tLoss: 0.0191 Acc@1: 0.540\n",
      "[2018-06-13 04:01:36] Testing Epoch [91/200] Iter[124/125]\t\tLoss: 0.0216 Acc@1: 0.475\n",
      "Epoch #91 Cost 241s\n",
      "Training Epoch: #92, LR: 0.0200\n",
      "[2018-06-13 04:05:26] Training Epoch [92/200] Iter[1041/1041]\t\tLoss: 0.0191 Acc@1: 0.540\n",
      "[2018-06-13 04:05:36] Testing Epoch [92/200] Iter[124/125]\t\tLoss: 0.0213 Acc@1: 0.482\n",
      "Epoch #92 Cost 240s\n",
      "Training Epoch: #93, LR: 0.0200\n",
      "[2018-06-13 04:09:27] Training Epoch [93/200] Iter[1041/1041]\t\tLoss: 0.0191 Acc@1: 0.540\n",
      "[2018-06-13 04:09:37] Testing Epoch [93/200] Iter[124/125]\t\tLoss: 0.0211 Acc@1: 0.491\n",
      "Epoch #93 Cost 240s\n",
      "Training Epoch: #94, LR: 0.0200\n",
      "[2018-06-13 04:13:27] Training Epoch [94/200] Iter[1041/1041]\t\tLoss: 0.0190 Acc@1: 0.542\n",
      "[2018-06-13 04:13:37] Testing Epoch [94/200] Iter[124/125]\t\tLoss: 0.0218 Acc@1: 0.472\n",
      "Epoch #94 Cost 239s\n",
      "Training Epoch: #95, LR: 0.0200\n",
      "[2018-06-13 04:17:28] Training Epoch [95/200] Iter[1041/1041]\t\tLoss: 0.0190 Acc@1: 0.540\n",
      "[2018-06-13 04:17:38] Testing Epoch [95/200] Iter[124/125]\t\tLoss: 0.0213 Acc@1: 0.480\n",
      "Epoch #95 Cost 241s\n",
      "Training Epoch: #96, LR: 0.0200\n",
      "[2018-06-13 04:21:29] Training Epoch [96/200] Iter[1041/1041]\t\tLoss: 0.0191 Acc@1: 0.541\n",
      "[2018-06-13 04:21:39] Testing Epoch [96/200] Iter[124/125]\t\tLoss: 0.0214 Acc@1: 0.475\n",
      "Epoch #96 Cost 241s\n",
      "Training Epoch: #97, LR: 0.0200\n",
      "[2018-06-13 04:25:30] Training Epoch [97/200] Iter[1041/1041]\t\tLoss: 0.0190 Acc@1: 0.541\n",
      "[2018-06-13 04:25:40] Testing Epoch [97/200] Iter[124/125]\t\tLoss: 0.0208 Acc@1: 0.494\n",
      "Epoch #97 Cost 240s\n",
      "Training Epoch: #98, LR: 0.0200\n",
      "[2018-06-13 04:29:29] Training Epoch [98/200] Iter[1041/1041]\t\tLoss: 0.0190 Acc@1: 0.543\n",
      "[2018-06-13 04:29:39] Testing Epoch [98/200] Iter[124/125]\t\tLoss: 0.0231 Acc@1: 0.444\n",
      "Epoch #98 Cost 239s\n",
      "Training Epoch: #99, LR: 0.0200\n",
      "[2018-06-13 04:33:30] Training Epoch [99/200] Iter[1041/1041]\t\tLoss: 0.0190 Acc@1: 0.542\n",
      "[2018-06-13 04:33:40] Testing Epoch [99/200] Iter[124/125]\t\tLoss: 0.0216 Acc@1: 0.478\n",
      "Epoch #99 Cost 240s\n",
      "Training Epoch: #100, LR: 0.0200\n",
      "[2018-06-13 04:37:31] Training Epoch [100/200] Iter[1041/1041]\t\tLoss: 0.0190 Acc@1: 0.543\n",
      "[2018-06-13 04:37:41] Testing Epoch [100/200] Iter[124/125]\t\tLoss: 0.0223 Acc@1: 0.462\n",
      "Epoch #100 Cost 241s\n",
      "Training Epoch: #101, LR: 0.0200\n",
      "[2018-06-13 04:41:31] Training Epoch [101/200] Iter[1041/1041]\t\tLoss: 0.0189 Acc@1: 0.543\n",
      "[2018-06-13 04:41:41] Testing Epoch [101/200] Iter[124/125]\t\tLoss: 0.0220 Acc@1: 0.470\n",
      "Epoch #101 Cost 239s\n",
      "Training Epoch: #102, LR: 0.0200\n",
      "[2018-06-13 04:45:30] Training Epoch [102/200] Iter[1041/1041]\t\tLoss: 0.0190 Acc@1: 0.542\n",
      "[2018-06-13 04:45:40] Testing Epoch [102/200] Iter[124/125]\t\tLoss: 0.0219 Acc@1: 0.475\n",
      "Epoch #102 Cost 239s\n",
      "Training Epoch: #103, LR: 0.0200\n",
      "[2018-06-13 04:49:31] Training Epoch [103/200] Iter[1041/1041]\t\tLoss: 0.0189 Acc@1: 0.543\n",
      "[2018-06-13 04:49:41] Testing Epoch [103/200] Iter[124/125]\t\tLoss: 0.0213 Acc@1: 0.486\n",
      "Epoch #103 Cost 240s\n",
      "Training Epoch: #104, LR: 0.0200\n",
      "[2018-06-13 04:53:33] Training Epoch [104/200] Iter[1041/1041]\t\tLoss: 0.0189 Acc@1: 0.543\n",
      "[2018-06-13 04:53:43] Testing Epoch [104/200] Iter[124/125]\t\tLoss: 0.0225 Acc@1: 0.455\n",
      "Epoch #104 Cost 241s\n",
      "Training Epoch: #105, LR: 0.0200\n",
      "[2018-06-13 04:57:32] Training Epoch [105/200] Iter[1041/1041]\t\tLoss: 0.0190 Acc@1: 0.542\n",
      "[2018-06-13 04:57:42] Testing Epoch [105/200] Iter[124/125]\t\tLoss: 0.0213 Acc@1: 0.486\n",
      "Epoch #105 Cost 239s\n",
      "Training Epoch: #106, LR: 0.0200\n",
      "[2018-06-13 05:01:32] Training Epoch [106/200] Iter[1041/1041]\t\tLoss: 0.0189 Acc@1: 0.543\n",
      "[2018-06-13 05:01:42] Testing Epoch [106/200] Iter[124/125]\t\tLoss: 0.0216 Acc@1: 0.479\n",
      "Epoch #106 Cost 240s\n",
      "Training Epoch: #107, LR: 0.0200\n",
      "[2018-06-13 05:05:33] Training Epoch [107/200] Iter[1041/1041]\t\tLoss: 0.0189 Acc@1: 0.543\n",
      "[2018-06-13 05:05:43] Testing Epoch [107/200] Iter[124/125]\t\tLoss: 0.0211 Acc@1: 0.488\n",
      "Epoch #107 Cost 240s\n",
      "Training Epoch: #108, LR: 0.0200\n",
      "[2018-06-13 05:09:32] Training Epoch [108/200] Iter[1041/1041]\t\tLoss: 0.0189 Acc@1: 0.543\n",
      "[2018-06-13 05:09:42] Testing Epoch [108/200] Iter[124/125]\t\tLoss: 0.0215 Acc@1: 0.479\n",
      "Epoch #108 Cost 239s\n",
      "Training Epoch: #109, LR: 0.0200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-13 05:13:33] Training Epoch [109/200] Iter[1041/1041]\t\tLoss: 0.0189 Acc@1: 0.543\n",
      "[2018-06-13 05:13:43] Testing Epoch [109/200] Iter[124/125]\t\tLoss: 0.0213 Acc@1: 0.483\n",
      "Epoch #109 Cost 241s\n",
      "Training Epoch: #110, LR: 0.0200\n",
      "[2018-06-13 05:17:34] Training Epoch [110/200] Iter[1041/1041]\t\tLoss: 0.0188 Acc@1: 0.545\n",
      "[2018-06-13 05:17:44] Testing Epoch [110/200] Iter[124/125]\t\tLoss: 0.0216 Acc@1: 0.481\n",
      "Epoch #110 Cost 240s\n",
      "Training Epoch: #111, LR: 0.0200\n",
      "[2018-06-13 05:21:35] Training Epoch [111/200] Iter[1041/1041]\t\tLoss: 0.0189 Acc@1: 0.544\n",
      "[2018-06-13 05:21:45] Testing Epoch [111/200] Iter[124/125]\t\tLoss: 0.0209 Acc@1: 0.493\n",
      "Epoch #111 Cost 240s\n",
      "Training Epoch: #112, LR: 0.0200\n",
      "[2018-06-13 05:25:34] Training Epoch [112/200] Iter[1041/1041]\t\tLoss: 0.0188 Acc@1: 0.545\n",
      "[2018-06-13 05:25:44] Testing Epoch [112/200] Iter[124/125]\t\tLoss: 0.0213 Acc@1: 0.479\n",
      "Epoch #112 Cost 239s\n",
      "Training Epoch: #113, LR: 0.0200\n",
      "[2018-06-13 05:29:35] Training Epoch [113/200] Iter[1041/1041]\t\tLoss: 0.0188 Acc@1: 0.544\n",
      "[2018-06-13 05:29:45] Testing Epoch [113/200] Iter[124/125]\t\tLoss: 0.0211 Acc@1: 0.490\n",
      "Epoch #113 Cost 240s\n",
      "Training Epoch: #114, LR: 0.0200\n",
      "[2018-06-13 05:33:35] Training Epoch [114/200] Iter[1041/1041]\t\tLoss: 0.0188 Acc@1: 0.545\n",
      "[2018-06-13 05:33:45] Testing Epoch [114/200] Iter[124/125]\t\tLoss: 0.0215 Acc@1: 0.481\n",
      "Epoch #114 Cost 240s\n",
      "Training Epoch: #115, LR: 0.0200\n",
      "[2018-06-13 05:37:36] Training Epoch [115/200] Iter[1041/1041]\t\tLoss: 0.0188 Acc@1: 0.545\n",
      "[2018-06-13 05:37:46] Testing Epoch [115/200] Iter[124/125]\t\tLoss: 0.0214 Acc@1: 0.483\n",
      "Epoch #115 Cost 240s\n",
      "Training Epoch: #116, LR: 0.0200\n",
      "[2018-06-13 05:41:36] Training Epoch [116/200] Iter[1041/1041]\t\tLoss: 0.0188 Acc@1: 0.547\n",
      "[2018-06-13 05:41:46] Testing Epoch [116/200] Iter[124/125]\t\tLoss: 0.0219 Acc@1: 0.480\n",
      "Epoch #116 Cost 240s\n",
      "Training Epoch: #117, LR: 0.0200\n",
      "[2018-06-13 05:45:34] Training Epoch [117/200] Iter[1041/1041]\t\tLoss: 0.0188 Acc@1: 0.547\n",
      "[2018-06-13 05:45:44] Testing Epoch [117/200] Iter[124/125]\t\tLoss: 0.0209 Acc@1: 0.492\n",
      "Epoch #117 Cost 238s\n",
      "Training Epoch: #118, LR: 0.0200\n",
      "[2018-06-13 05:49:34] Training Epoch [118/200] Iter[1041/1041]\t\tLoss: 0.0188 Acc@1: 0.545\n",
      "[2018-06-13 05:49:44] Testing Epoch [118/200] Iter[124/125]\t\tLoss: 0.0217 Acc@1: 0.474\n",
      "Epoch #118 Cost 239s\n",
      "Training Epoch: #119, LR: 0.0200\n",
      "[2018-06-13 05:53:34] Training Epoch [119/200] Iter[1041/1041]\t\tLoss: 0.0188 Acc@1: 0.545\n",
      "[2018-06-13 05:53:44] Testing Epoch [119/200] Iter[124/125]\t\tLoss: 0.0214 Acc@1: 0.480\n",
      "Epoch #119 Cost 240s\n",
      "Training Epoch: #120, LR: 0.0200\n",
      "[2018-06-13 05:57:35] Training Epoch [120/200] Iter[1041/1041]\t\tLoss: 0.0188 Acc@1: 0.545\n",
      "[2018-06-13 05:57:45] Testing Epoch [120/200] Iter[124/125]\t\tLoss: 0.0213 Acc@1: 0.487\n",
      "Epoch #120 Cost 240s\n",
      "Training Epoch: #121, LR: 0.0040\n",
      "[2018-06-13 06:01:35] Training Epoch [121/200] Iter[1041/1041]\t\tLoss: 0.0157 Acc@1: 0.619\n",
      "[2018-06-13 06:01:45] Testing Epoch [121/200] Iter[124/125]\t\tLoss: 0.0176 Acc@1: 0.568\n",
      "Saving Model...\n",
      "\n",
      "Epoch #121 Cost 240s\n",
      "Training Epoch: #122, LR: 0.0040\n",
      "[2018-06-13 06:05:33] Training Epoch [122/200] Iter[1041/1041]\t\tLoss: 0.0149 Acc@1: 0.636\n",
      "[2018-06-13 06:05:43] Testing Epoch [122/200] Iter[124/125]\t\tLoss: 0.0175 Acc@1: 0.568\n",
      "Epoch #122 Cost 238s\n",
      "Training Epoch: #123, LR: 0.0040\n",
      "[2018-06-13 06:09:33] Training Epoch [123/200] Iter[1041/1041]\t\tLoss: 0.0146 Acc@1: 0.643\n",
      "[2018-06-13 06:09:44] Testing Epoch [123/200] Iter[124/125]\t\tLoss: 0.0177 Acc@1: 0.568\n",
      "Epoch #123 Cost 240s\n",
      "Training Epoch: #124, LR: 0.0040\n",
      "[2018-06-13 06:13:33] Training Epoch [124/200] Iter[1041/1041]\t\tLoss: 0.0145 Acc@1: 0.646\n",
      "[2018-06-13 06:13:43] Testing Epoch [124/200] Iter[124/125]\t\tLoss: 0.0175 Acc@1: 0.570\n",
      "Saving Model...\n",
      "\n",
      "Epoch #124 Cost 239s\n",
      "Training Epoch: #125, LR: 0.0040\n",
      "[2018-06-13 06:17:33] Training Epoch [125/200] Iter[1041/1041]\t\tLoss: 0.0143 Acc@1: 0.647\n",
      "[2018-06-13 06:17:43] Testing Epoch [125/200] Iter[124/125]\t\tLoss: 0.0176 Acc@1: 0.571\n",
      "Saving Model...\n",
      "\n",
      "Epoch #125 Cost 240s\n",
      "Training Epoch: #126, LR: 0.0040\n",
      "[2018-06-13 06:21:34] Training Epoch [126/200] Iter[1041/1041]\t\tLoss: 0.0142 Acc@1: 0.651\n",
      "[2018-06-13 06:21:44] Testing Epoch [126/200] Iter[124/125]\t\tLoss: 0.0177 Acc@1: 0.570\n",
      "Epoch #126 Cost 240s\n",
      "Training Epoch: #127, LR: 0.0040\n",
      "[2018-06-13 06:25:34] Training Epoch [127/200] Iter[1041/1041]\t\tLoss: 0.0142 Acc@1: 0.651\n",
      "[2018-06-13 06:25:44] Testing Epoch [127/200] Iter[124/125]\t\tLoss: 0.0177 Acc@1: 0.565\n",
      "Epoch #127 Cost 240s\n",
      "Training Epoch: #128, LR: 0.0040\n",
      "[2018-06-13 06:29:34] Training Epoch [128/200] Iter[1041/1041]\t\tLoss: 0.0141 Acc@1: 0.653\n",
      "[2018-06-13 06:29:44] Testing Epoch [128/200] Iter[124/125]\t\tLoss: 0.0177 Acc@1: 0.569\n",
      "Epoch #128 Cost 239s\n",
      "Training Epoch: #129, LR: 0.0040\n",
      "[2018-06-13 06:33:32] Training Epoch [129/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.653\n",
      "[2018-06-13 06:33:43] Testing Epoch [129/200] Iter[124/125]\t\tLoss: 0.0178 Acc@1: 0.563\n",
      "Epoch #129 Cost 238s\n",
      "Training Epoch: #130, LR: 0.0040\n",
      "[2018-06-13 06:37:31] Training Epoch [130/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.653\n",
      "[2018-06-13 06:37:41] Testing Epoch [130/200] Iter[124/125]\t\tLoss: 0.0177 Acc@1: 0.565\n",
      "Epoch #130 Cost 238s\n",
      "Training Epoch: #131, LR: 0.0040\n",
      "[2018-06-13 06:41:31] Training Epoch [131/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.653\n",
      "[2018-06-13 06:41:41] Testing Epoch [131/200] Iter[124/125]\t\tLoss: 0.0178 Acc@1: 0.568\n",
      "Epoch #131 Cost 240s\n",
      "Training Epoch: #132, LR: 0.0040\n",
      "[2018-06-13 06:45:30] Training Epoch [132/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.655\n",
      "[2018-06-13 06:45:40] Testing Epoch [132/200] Iter[124/125]\t\tLoss: 0.0179 Acc@1: 0.559\n",
      "Epoch #132 Cost 239s\n",
      "Training Epoch: #133, LR: 0.0040\n",
      "[2018-06-13 06:49:30] Training Epoch [133/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.654\n",
      "[2018-06-13 06:49:40] Testing Epoch [133/200] Iter[124/125]\t\tLoss: 0.0180 Acc@1: 0.557\n",
      "Epoch #133 Cost 239s\n",
      "Training Epoch: #134, LR: 0.0040\n",
      "[2018-06-13 06:53:30] Training Epoch [134/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.654\n",
      "[2018-06-13 06:53:40] Testing Epoch [134/200] Iter[124/125]\t\tLoss: 0.0181 Acc@1: 0.558\n",
      "Epoch #134 Cost 239s\n",
      "Training Epoch: #135, LR: 0.0040\n",
      "[2018-06-13 06:57:30] Training Epoch [135/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.654\n",
      "[2018-06-13 06:57:40] Testing Epoch [135/200] Iter[124/125]\t\tLoss: 0.0180 Acc@1: 0.561\n",
      "Epoch #135 Cost 239s\n",
      "Training Epoch: #136, LR: 0.0040\n",
      "[2018-06-13 07:01:29] Training Epoch [136/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.652\n",
      "[2018-06-13 07:01:39] Testing Epoch [136/200] Iter[124/125]\t\tLoss: 0.0179 Acc@1: 0.563\n",
      "Epoch #136 Cost 238s\n",
      "Training Epoch: #137, LR: 0.0040\n",
      "[2018-06-13 07:05:27] Training Epoch [137/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.652\n",
      "[2018-06-13 07:05:37] Testing Epoch [137/200] Iter[124/125]\t\tLoss: 0.0180 Acc@1: 0.557\n",
      "Epoch #137 Cost 238s\n",
      "Training Epoch: #138, LR: 0.0040\n",
      "[2018-06-13 07:09:27] Training Epoch [138/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.654\n",
      "[2018-06-13 07:09:37] Testing Epoch [138/200] Iter[124/125]\t\tLoss: 0.0180 Acc@1: 0.559\n",
      "Epoch #138 Cost 239s\n",
      "Training Epoch: #139, LR: 0.0040\n",
      "[2018-06-13 07:13:28] Training Epoch [139/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.652\n",
      "[2018-06-13 07:13:38] Testing Epoch [139/200] Iter[124/125]\t\tLoss: 0.0183 Acc@1: 0.550\n",
      "Epoch #139 Cost 240s\n",
      "Training Epoch: #140, LR: 0.0040\n",
      "[2018-06-13 07:17:28] Training Epoch [140/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.651\n",
      "[2018-06-13 07:17:38] Testing Epoch [140/200] Iter[124/125]\t\tLoss: 0.0182 Acc@1: 0.554\n",
      "Epoch #140 Cost 239s\n",
      "Training Epoch: #141, LR: 0.0040\n",
      "[2018-06-13 07:21:28] Training Epoch [141/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.653\n",
      "[2018-06-13 07:21:38] Testing Epoch [141/200] Iter[124/125]\t\tLoss: 0.0184 Acc@1: 0.557\n",
      "Epoch #141 Cost 240s\n",
      "Training Epoch: #142, LR: 0.0040\n",
      "[2018-06-13 07:25:28] Training Epoch [142/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.654\n",
      "[2018-06-13 07:25:38] Testing Epoch [142/200] Iter[124/125]\t\tLoss: 0.0183 Acc@1: 0.550\n",
      "Epoch #142 Cost 240s\n",
      "Training Epoch: #143, LR: 0.0040\n",
      "[2018-06-13 07:29:27] Training Epoch [143/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.654\n",
      "[2018-06-13 07:29:37] Testing Epoch [143/200] Iter[124/125]\t\tLoss: 0.0183 Acc@1: 0.558\n",
      "Epoch #143 Cost 238s\n",
      "Training Epoch: #144, LR: 0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-13 07:33:27] Training Epoch [144/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.652\n",
      "[2018-06-13 07:33:37] Testing Epoch [144/200] Iter[124/125]\t\tLoss: 0.0185 Acc@1: 0.552\n",
      "Epoch #144 Cost 240s\n",
      "Training Epoch: #145, LR: 0.0040\n",
      "[2018-06-13 07:37:27] Training Epoch [145/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.653\n",
      "[2018-06-13 07:37:38] Testing Epoch [145/200] Iter[124/125]\t\tLoss: 0.0183 Acc@1: 0.551\n",
      "Epoch #145 Cost 240s\n",
      "Training Epoch: #146, LR: 0.0040\n",
      "[2018-06-13 07:41:28] Training Epoch [146/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.651\n",
      "[2018-06-13 07:41:38] Testing Epoch [146/200] Iter[124/125]\t\tLoss: 0.0183 Acc@1: 0.554\n",
      "Epoch #146 Cost 240s\n",
      "Training Epoch: #147, LR: 0.0040\n",
      "[2018-06-13 07:45:28] Training Epoch [147/200] Iter[1041/1041]\t\tLoss: 0.0139 Acc@1: 0.653\n",
      "[2018-06-13 07:45:38] Testing Epoch [147/200] Iter[124/125]\t\tLoss: 0.0184 Acc@1: 0.555\n",
      "Epoch #147 Cost 240s\n",
      "Training Epoch: #148, LR: 0.0040\n",
      "[2018-06-13 07:49:28] Training Epoch [148/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.653\n",
      "[2018-06-13 07:49:38] Testing Epoch [148/200] Iter[124/125]\t\tLoss: 0.0183 Acc@1: 0.554\n",
      "Epoch #148 Cost 239s\n",
      "Training Epoch: #149, LR: 0.0040\n",
      "[2018-06-13 07:53:28] Training Epoch [149/200] Iter[1041/1041]\t\tLoss: 0.0139 Acc@1: 0.653\n",
      "[2018-06-13 07:53:38] Testing Epoch [149/200] Iter[124/125]\t\tLoss: 0.0182 Acc@1: 0.561\n",
      "Epoch #149 Cost 240s\n",
      "Training Epoch: #150, LR: 0.0040\n",
      "[2018-06-13 07:57:28] Training Epoch [150/200] Iter[1041/1041]\t\tLoss: 0.0139 Acc@1: 0.653\n",
      "[2018-06-13 07:57:38] Testing Epoch [150/200] Iter[124/125]\t\tLoss: 0.0183 Acc@1: 0.554\n",
      "Epoch #150 Cost 240s\n",
      "Training Epoch: #151, LR: 0.0040\n",
      "[2018-06-13 08:01:30] Training Epoch [151/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.650\n",
      "[2018-06-13 08:01:40] Testing Epoch [151/200] Iter[124/125]\t\tLoss: 0.0184 Acc@1: 0.554\n",
      "Epoch #151 Cost 241s\n",
      "Training Epoch: #152, LR: 0.0040\n",
      "[2018-06-13 08:05:25] Training Epoch [152/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.653\n",
      "[2018-06-13 08:05:35] Testing Epoch [152/200] Iter[124/125]\t\tLoss: 0.0187 Acc@1: 0.541\n",
      "Epoch #152 Cost 235s\n",
      "Training Epoch: #153, LR: 0.0040\n",
      "[2018-06-13 08:09:21] Training Epoch [153/200] Iter[1041/1041]\t\tLoss: 0.0139 Acc@1: 0.651\n",
      "[2018-06-13 08:09:31] Testing Epoch [153/200] Iter[124/125]\t\tLoss: 0.0188 Acc@1: 0.544\n",
      "Epoch #153 Cost 235s\n",
      "Training Epoch: #154, LR: 0.0040\n",
      "[2018-06-13 08:13:16] Training Epoch [154/200] Iter[1041/1041]\t\tLoss: 0.0139 Acc@1: 0.653\n",
      "[2018-06-13 08:13:26] Testing Epoch [154/200] Iter[124/125]\t\tLoss: 0.0186 Acc@1: 0.551\n",
      "Epoch #154 Cost 235s\n",
      "Training Epoch: #155, LR: 0.0040\n",
      "[2018-06-13 08:17:12] Training Epoch [155/200] Iter[1041/1041]\t\tLoss: 0.0139 Acc@1: 0.653\n",
      "[2018-06-13 08:17:22] Testing Epoch [155/200] Iter[124/125]\t\tLoss: 0.0189 Acc@1: 0.543\n",
      "Epoch #155 Cost 235s\n",
      "Training Epoch: #156, LR: 0.0040\n",
      "[2018-06-13 08:21:10] Training Epoch [156/200] Iter[1041/1041]\t\tLoss: 0.0140 Acc@1: 0.653\n",
      "[2018-06-13 08:21:20] Testing Epoch [156/200] Iter[124/125]\t\tLoss: 0.0186 Acc@1: 0.549\n",
      "Epoch #156 Cost 237s\n",
      "Training Epoch: #157, LR: 0.0040\n",
      "[2018-06-13 08:25:08] Training Epoch [157/200] Iter[1041/1041]\t\tLoss: 0.0139 Acc@1: 0.653\n",
      "[2018-06-13 08:25:18] Testing Epoch [157/200] Iter[124/125]\t\tLoss: 0.0186 Acc@1: 0.547\n",
      "Epoch #157 Cost 238s\n",
      "Training Epoch: #158, LR: 0.0040\n",
      "[2018-06-13 08:29:07] Training Epoch [158/200] Iter[1041/1041]\t\tLoss: 0.0139 Acc@1: 0.651\n",
      "[2018-06-13 08:29:17] Testing Epoch [158/200] Iter[124/125]\t\tLoss: 0.0185 Acc@1: 0.549\n",
      "Epoch #158 Cost 238s\n",
      "Training Epoch: #159, LR: 0.0040\n",
      "[2018-06-13 08:33:04] Training Epoch [159/200] Iter[1041/1041]\t\tLoss: 0.0139 Acc@1: 0.654\n",
      "[2018-06-13 08:33:14] Testing Epoch [159/200] Iter[124/125]\t\tLoss: 0.0185 Acc@1: 0.552\n",
      "Epoch #159 Cost 237s\n",
      "Training Epoch: #160, LR: 0.0040\n",
      "[2018-06-13 08:37:03] Training Epoch [160/200] Iter[1041/1041]\t\tLoss: 0.0139 Acc@1: 0.653\n",
      "[2018-06-13 08:37:13] Testing Epoch [160/200] Iter[124/125]\t\tLoss: 0.0186 Acc@1: 0.548\n",
      "Epoch #160 Cost 238s\n",
      "Training Epoch: #161, LR: 0.0008\n",
      "[2018-06-13 08:41:01] Training Epoch [161/200] Iter[1041/1041]\t\tLoss: 0.0122 Acc@1: 0.699\n",
      "[2018-06-13 08:41:11] Testing Epoch [161/200] Iter[124/125]\t\tLoss: 0.0171 Acc@1: 0.581\n",
      "Saving Model...\n",
      "\n",
      "Epoch #161 Cost 238s\n",
      "Training Epoch: #162, LR: 0.0008\n",
      "[2018-06-13 08:44:59] Training Epoch [162/200] Iter[1041/1041]\t\tLoss: 0.0117 Acc@1: 0.712\n",
      "[2018-06-13 08:45:09] Testing Epoch [162/200] Iter[124/125]\t\tLoss: 0.0171 Acc@1: 0.582\n",
      "Saving Model...\n",
      "\n",
      "Epoch #162 Cost 238s\n",
      "Training Epoch: #163, LR: 0.0008\n",
      "[2018-06-13 08:48:58] Training Epoch [163/200] Iter[1041/1041]\t\tLoss: 0.0116 Acc@1: 0.715\n",
      "[2018-06-13 08:49:08] Testing Epoch [163/200] Iter[124/125]\t\tLoss: 0.0171 Acc@1: 0.581\n",
      "Epoch #163 Cost 238s\n",
      "Training Epoch: #164, LR: 0.0008\n",
      "[2018-06-13 08:52:55] Training Epoch [164/200] Iter[1041/1041]\t\tLoss: 0.0115 Acc@1: 0.717\n",
      "[2018-06-13 08:53:06] Testing Epoch [164/200] Iter[124/125]\t\tLoss: 0.0172 Acc@1: 0.580\n",
      "Epoch #164 Cost 237s\n",
      "Training Epoch: #165, LR: 0.0008\n",
      "[2018-06-13 08:56:54] Training Epoch [165/200] Iter[1041/1041]\t\tLoss: 0.0114 Acc@1: 0.717\n",
      "[2018-06-13 08:57:04] Testing Epoch [165/200] Iter[124/125]\t\tLoss: 0.0172 Acc@1: 0.579\n",
      "Epoch #165 Cost 238s\n",
      "Training Epoch: #166, LR: 0.0008\n",
      "[2018-06-13 09:00:52] Training Epoch [166/200] Iter[1041/1041]\t\tLoss: 0.0114 Acc@1: 0.718\n",
      "[2018-06-13 09:01:02] Testing Epoch [166/200] Iter[124/125]\t\tLoss: 0.0172 Acc@1: 0.579\n",
      "Epoch #166 Cost 238s\n",
      "Training Epoch: #167, LR: 0.0008\n",
      "[2018-06-13 09:04:51] Training Epoch [167/200] Iter[1041/1041]\t\tLoss: 0.0113 Acc@1: 0.719\n",
      "[2018-06-13 09:05:01] Testing Epoch [167/200] Iter[124/125]\t\tLoss: 0.0172 Acc@1: 0.579\n",
      "Epoch #167 Cost 238s\n",
      "Training Epoch: #168, LR: 0.0008\n",
      "[2018-06-13 09:08:51] Training Epoch [168/200] Iter[1041/1041]\t\tLoss: 0.0113 Acc@1: 0.722\n",
      "[2018-06-13 09:09:01] Testing Epoch [168/200] Iter[124/125]\t\tLoss: 0.0173 Acc@1: 0.576\n",
      "Epoch #168 Cost 239s\n",
      "Training Epoch: #169, LR: 0.0008\n",
      "[2018-06-13 09:12:52] Training Epoch [169/200] Iter[1041/1041]\t\tLoss: 0.0112 Acc@1: 0.720\n",
      "[2018-06-13 09:13:02] Testing Epoch [169/200] Iter[124/125]\t\tLoss: 0.0174 Acc@1: 0.578\n",
      "Epoch #169 Cost 240s\n",
      "Training Epoch: #170, LR: 0.0008\n",
      "[2018-06-13 09:16:53] Training Epoch [170/200] Iter[1041/1041]\t\tLoss: 0.0112 Acc@1: 0.721\n",
      "[2018-06-13 09:17:03] Testing Epoch [170/200] Iter[124/125]\t\tLoss: 0.0173 Acc@1: 0.578\n",
      "Epoch #170 Cost 240s\n",
      "Training Epoch: #171, LR: 0.0008\n",
      "[2018-06-13 09:20:54] Training Epoch [171/200] Iter[1041/1041]\t\tLoss: 0.0112 Acc@1: 0.723\n",
      "[2018-06-13 09:21:04] Testing Epoch [171/200] Iter[124/125]\t\tLoss: 0.0174 Acc@1: 0.577\n",
      "Epoch #171 Cost 241s\n",
      "Training Epoch: #172, LR: 0.0008\n",
      "[2018-06-13 09:25:24] Training Epoch [172/200] Iter[1041/1041]\t\tLoss: 0.0112 Acc@1: 0.722\n",
      "[2018-06-13 09:25:36] Testing Epoch [172/200] Iter[124/125]\t\tLoss: 0.0175 Acc@1: 0.578\n",
      "Epoch #172 Cost 271s\n",
      "Training Epoch: #173, LR: 0.0008\n",
      "[2018-06-13 09:29:27] Training Epoch [173/200] Iter[1041/1041]\t\tLoss: 0.0111 Acc@1: 0.724\n",
      "[2018-06-13 09:29:38] Testing Epoch [173/200] Iter[124/125]\t\tLoss: 0.0175 Acc@1: 0.577\n",
      "Epoch #173 Cost 242s\n",
      "Training Epoch: #174, LR: 0.0008\n",
      "[2018-06-13 09:33:29] Training Epoch [174/200] Iter[1041/1041]\t\tLoss: 0.0111 Acc@1: 0.725\n",
      "[2018-06-13 09:33:39] Testing Epoch [174/200] Iter[124/125]\t\tLoss: 0.0174 Acc@1: 0.580\n",
      "Epoch #174 Cost 241s\n",
      "Training Epoch: #175, LR: 0.0008\n",
      "[2018-06-13 09:37:32] Training Epoch [175/200] Iter[1041/1041]\t\tLoss: 0.0111 Acc@1: 0.725\n",
      "[2018-06-13 09:37:42] Testing Epoch [175/200] Iter[124/125]\t\tLoss: 0.0174 Acc@1: 0.580\n",
      "Epoch #175 Cost 243s\n",
      "Training Epoch: #176, LR: 0.0008\n",
      "[2018-06-13 09:41:34] Training Epoch [176/200] Iter[1041/1041]\t\tLoss: 0.0111 Acc@1: 0.726\n",
      "[2018-06-13 09:41:45] Testing Epoch [176/200] Iter[124/125]\t\tLoss: 0.0175 Acc@1: 0.576\n",
      "Epoch #176 Cost 242s\n",
      "Training Epoch: #177, LR: 0.0008\n",
      "[2018-06-13 09:45:37] Training Epoch [177/200] Iter[1041/1041]\t\tLoss: 0.0110 Acc@1: 0.725\n",
      "[2018-06-13 09:45:47] Testing Epoch [177/200] Iter[124/125]\t\tLoss: 0.0175 Acc@1: 0.575\n",
      "Epoch #177 Cost 242s\n",
      "Training Epoch: #178, LR: 0.0008\n",
      "[2018-06-13 09:49:41] Training Epoch [178/200] Iter[1041/1041]\t\tLoss: 0.0110 Acc@1: 0.724\n",
      "[2018-06-13 09:49:52] Testing Epoch [178/200] Iter[124/125]\t\tLoss: 0.0175 Acc@1: 0.576\n",
      "Epoch #178 Cost 244s\n",
      "Training Epoch: #179, LR: 0.0008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-13 09:53:45] Training Epoch [179/200] Iter[1041/1041]\t\tLoss: 0.0110 Acc@1: 0.726\n",
      "[2018-06-13 09:53:55] Testing Epoch [179/200] Iter[124/125]\t\tLoss: 0.0175 Acc@1: 0.576\n",
      "Epoch #179 Cost 243s\n",
      "Training Epoch: #180, LR: 0.0008\n",
      "[2018-06-13 09:57:48] Training Epoch [180/200] Iter[1041/1041]\t\tLoss: 0.0110 Acc@1: 0.726\n",
      "[2018-06-13 09:57:59] Testing Epoch [180/200] Iter[124/125]\t\tLoss: 0.0175 Acc@1: 0.577\n",
      "Epoch #180 Cost 243s\n",
      "Training Epoch: #181, LR: 0.0008\n",
      "[2018-06-13 10:01:49] Training Epoch [181/200] Iter[1041/1041]\t\tLoss: 0.0110 Acc@1: 0.725\n",
      "[2018-06-13 10:01:59] Testing Epoch [181/200] Iter[124/125]\t\tLoss: 0.0176 Acc@1: 0.574\n",
      "Epoch #181 Cost 240s\n",
      "Training Epoch: #182, LR: 0.0008\n",
      "[2018-06-13 10:05:49] Training Epoch [182/200] Iter[1041/1041]\t\tLoss: 0.0110 Acc@1: 0.725\n",
      "[2018-06-13 10:05:59] Testing Epoch [182/200] Iter[124/125]\t\tLoss: 0.0176 Acc@1: 0.575\n",
      "Epoch #182 Cost 239s\n",
      "Training Epoch: #183, LR: 0.0008\n",
      "[2018-06-13 10:09:49] Training Epoch [183/200] Iter[1041/1041]\t\tLoss: 0.0109 Acc@1: 0.727\n",
      "[2018-06-13 10:09:59] Testing Epoch [183/200] Iter[124/125]\t\tLoss: 0.0177 Acc@1: 0.571\n",
      "Epoch #183 Cost 239s\n",
      "Training Epoch: #184, LR: 0.0008\n",
      "[2018-06-13 10:13:49] Training Epoch [184/200] Iter[1041/1041]\t\tLoss: 0.0109 Acc@1: 0.726\n",
      "[2018-06-13 10:13:59] Testing Epoch [184/200] Iter[124/125]\t\tLoss: 0.0176 Acc@1: 0.576\n",
      "Epoch #184 Cost 240s\n",
      "Training Epoch: #185, LR: 0.0008\n",
      "[2018-06-13 10:17:49] Training Epoch [185/200] Iter[1041/1041]\t\tLoss: 0.0109 Acc@1: 0.728\n",
      "[2018-06-13 10:18:00] Testing Epoch [185/200] Iter[124/125]\t\tLoss: 0.0177 Acc@1: 0.573\n",
      "Epoch #185 Cost 240s\n",
      "Training Epoch: #186, LR: 0.0008\n",
      "[2018-06-13 10:21:52] Training Epoch [186/200] Iter[1041/1041]\t\tLoss: 0.0109 Acc@1: 0.726\n",
      "[2018-06-13 10:22:02] Testing Epoch [186/200] Iter[124/125]\t\tLoss: 0.0177 Acc@1: 0.574\n",
      "Epoch #186 Cost 242s\n",
      "Training Epoch: #187, LR: 0.0008\n",
      "[2018-06-13 10:25:56] Training Epoch [187/200] Iter[1041/1041]\t\tLoss: 0.0108 Acc@1: 0.729\n",
      "[2018-06-13 10:26:06] Testing Epoch [187/200] Iter[124/125]\t\tLoss: 0.0176 Acc@1: 0.571\n",
      "Epoch #187 Cost 244s\n",
      "Training Epoch: #188, LR: 0.0008\n",
      "[2018-06-13 10:29:59] Training Epoch [188/200] Iter[1041/1041]\t\tLoss: 0.0108 Acc@1: 0.728\n",
      "[2018-06-13 10:30:09] Testing Epoch [188/200] Iter[124/125]\t\tLoss: 0.0176 Acc@1: 0.570\n",
      "Epoch #188 Cost 243s\n",
      "Training Epoch: #189, LR: 0.0008\n",
      "[2018-06-13 10:34:02] Training Epoch [189/200] Iter[1041/1041]\t\tLoss: 0.0108 Acc@1: 0.728\n",
      "[2018-06-13 10:34:12] Testing Epoch [189/200] Iter[124/125]\t\tLoss: 0.0177 Acc@1: 0.571\n",
      "Epoch #189 Cost 242s\n",
      "Training Epoch: #190, LR: 0.0008\n",
      "[2018-06-13 10:38:05] Training Epoch [190/200] Iter[1041/1041]\t\tLoss: 0.0109 Acc@1: 0.728\n",
      "[2018-06-13 10:38:15] Testing Epoch [190/200] Iter[124/125]\t\tLoss: 0.0178 Acc@1: 0.572\n",
      "Epoch #190 Cost 243s\n",
      "Training Epoch: #191, LR: 0.0008\n",
      "[2018-06-13 10:42:08] Training Epoch [191/200] Iter[1041/1041]\t\tLoss: 0.0108 Acc@1: 0.729\n",
      "[2018-06-13 10:42:18] Testing Epoch [191/200] Iter[124/125]\t\tLoss: 0.0177 Acc@1: 0.572\n",
      "Epoch #191 Cost 242s\n",
      "Training Epoch: #192, LR: 0.0008\n",
      "[2018-06-13 10:46:10] Training Epoch [192/200] Iter[1041/1041]\t\tLoss: 0.0108 Acc@1: 0.728\n",
      "[2018-06-13 10:46:20] Testing Epoch [192/200] Iter[124/125]\t\tLoss: 0.0178 Acc@1: 0.573\n",
      "Epoch #192 Cost 242s\n",
      "Training Epoch: #193, LR: 0.0008\n",
      "[2018-06-13 10:50:14] Training Epoch [193/200] Iter[1041/1041]\t\tLoss: 0.0108 Acc@1: 0.728\n",
      "[2018-06-13 10:50:24] Testing Epoch [193/200] Iter[124/125]\t\tLoss: 0.0177 Acc@1: 0.574\n",
      "Epoch #193 Cost 243s\n",
      "Training Epoch: #194, LR: 0.0008\n",
      "[2018-06-13 10:54:17] Training Epoch [194/200] Iter[1041/1041]\t\tLoss: 0.0108 Acc@1: 0.728\n",
      "[2018-06-13 10:54:27] Testing Epoch [194/200] Iter[124/125]\t\tLoss: 0.0178 Acc@1: 0.572\n",
      "Epoch #194 Cost 242s\n",
      "Training Epoch: #195, LR: 0.0008\n",
      "[2018-06-13 10:58:19] Training Epoch [195/200] Iter[1041/1041]\t\tLoss: 0.0108 Acc@1: 0.728\n",
      "[2018-06-13 10:58:29] Testing Epoch [195/200] Iter[124/125]\t\tLoss: 0.0178 Acc@1: 0.571\n",
      "Epoch #195 Cost 242s\n",
      "Training Epoch: #196, LR: 0.0008\n",
      "[2018-06-13 11:02:22] Training Epoch [196/200] Iter[1041/1041]\t\tLoss: 0.0108 Acc@1: 0.727\n",
      "[2018-06-13 11:02:32] Testing Epoch [196/200] Iter[124/125]\t\tLoss: 0.0179 Acc@1: 0.564\n",
      "Epoch #196 Cost 242s\n",
      "Training Epoch: #197, LR: 0.0008\n",
      "[2018-06-13 11:06:25] Training Epoch [197/200] Iter[1041/1041]\t\tLoss: 0.0108 Acc@1: 0.729\n",
      "[2018-06-13 11:06:35] Testing Epoch [197/200] Iter[124/125]\t\tLoss: 0.0177 Acc@1: 0.568\n",
      "Epoch #197 Cost 243s\n",
      "Training Epoch: #198, LR: 0.0008\n",
      "[2018-06-13 11:10:28] Training Epoch [198/200] Iter[1041/1041]\t\tLoss: 0.0108 Acc@1: 0.729\n",
      "[2018-06-13 11:10:38] Testing Epoch [198/200] Iter[124/125]\t\tLoss: 0.0178 Acc@1: 0.574\n",
      "Epoch #198 Cost 242s\n",
      "Training Epoch: #199, LR: 0.0008\n",
      "[2018-06-13 11:14:29] Training Epoch [199/200] Iter[1041/1041]\t\tLoss: 0.0107 Acc@1: 0.729\n",
      "[2018-06-13 11:14:39] Testing Epoch [199/200] Iter[124/125]\t\tLoss: 0.0178 Acc@1: 0.568\n",
      "Epoch #199 Cost 241s\n",
      "Training Epoch: #200, LR: 0.0008\n",
      "[2018-06-13 11:18:30] Training Epoch [200/200] Iter[1041/1041]\t\tLoss: 0.0107 Acc@1: 0.729\n",
      "[2018-06-13 11:18:40] Testing Epoch [200/200] Iter[124/125]\t\tLoss: 0.0178 Acc@1: 0.569\n",
      "Epoch #200 Cost 241s\n",
      "Best Acc@1: 58.1500\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for _epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    start_time = time.time()\n",
    "    train(_epoch)\n",
    "    print()\n",
    "    test(_epoch)\n",
    "    print()\n",
    "    end_time   = time.time()\n",
    "    print('Epoch #%d Cost %ds' % (_epoch, end_time - start_time))\n",
    "    \n",
    "print('Best Acc@1: %.4f' % (best_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
