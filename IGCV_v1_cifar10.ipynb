{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr           = 0.1\n",
    "\n",
    "start_epoch  = 1\n",
    "num_epochs   = 200\n",
    "batch_size   = 256\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "is_use_cuda = torch.cuda.is_available()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "best_acc    = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocess\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_test  = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', transform=transform_train, train=True, download=True)\n",
    "test_dataset  = torchvision.datasets.CIFAR10(root='./data', transform=transform_test, train=False, download=True)\n",
    "train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "test_loader   = torch.utils.data.DataLoader(test_dataset, batch_size=80, num_workers=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.3589 -0.1375  0.0552 -0.0505  0.0053  0.3871  0.1123  0.3889  0.0840 -0.1020\n",
      "[torch.FloatTensor of size 1x10]\n",
      " <class 'torch.autograd.variable.Variable'> torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "\n",
    "class ShuffleBlock(nn.Module):\n",
    "    def __init__(self, groups):\n",
    "        super(ShuffleBlock, self).__init__()\n",
    "        self.groups = groups\n",
    "        \n",
    "    def forward(self, input):\n",
    "        n, c, h, w = input.size()\n",
    "        G          = self.groups\n",
    "        output     = input.view(n, G, c // G, h, w).permute(0, 2, 1, 3, 4).contiguous().view(n, c, h, w)\n",
    "        return output\n",
    "\n",
    "class IGCVBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, L, M, stride):\n",
    "        super(IGCVBlock, self).__init__()\n",
    "        if out_channel != in_channel:\n",
    "            M_t = out_channel // L\n",
    "        else:\n",
    "            M_t = M\n",
    "        \n",
    "        self.conv1    = nn.Conv2d(in_channel, out_channel, 3, stride, 1, groups=L, bias=True)\n",
    "        self.bn1      = nn.BatchNorm2d(out_channel)\n",
    "        self.shuffle1 = ShuffleBlock(L)\n",
    "        self.conv2    = nn.Conv2d(out_channel, out_channel, 1, groups=M_t, bias=True)\n",
    "        self.bn2      = nn.BatchNorm2d(out_channel)\n",
    "        self.shuffle2 = ShuffleBlock(M_t)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if 1 != stride or in_channel != out_channel:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                            nn.Conv2d(in_channel, out_channel, 1, stride, bias=True),\n",
    "                            nn.BatchNorm2d(out_channel)\n",
    "            )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        feat = F.relu(self.bn1(self.conv1(input)))\n",
    "        feat = self.shuffle1(feat)\n",
    "        feat = self.bn2(self.conv2(feat))\n",
    "        feat = self.shuffle2(feat)\n",
    "        feat += self.shortcut(input)\n",
    "        feat = F.relu(feat)\n",
    "        return feat\n",
    "\n",
    "class IGCV_V1(nn.Module):\n",
    "    def __init__(self, L, M, D, is_L_twice, num_classes):\n",
    "        super(IGCV_V1, self).__init__()\n",
    "        \n",
    "        assert (D - 2) % 3 == 0, 'D must be equival to 3B + 2'\n",
    "        B = (D - 2) // 3\n",
    "        \n",
    "        self.in_channel = L * M\n",
    "        self.conv1  = nn.Conv2d(3, self.in_channel, 3, 1, 1, bias=True)\n",
    "        self.bn1    = nn.BatchNorm2d(self.in_channel)\n",
    "        self.stage1 = self._make_layers(self.in_channel, L, M, B)\n",
    "        self.in_channel *= 2\n",
    "        if is_L_twice:\n",
    "            L *= 2\n",
    "        else:\n",
    "            M *= 2\n",
    "        self.stage2 = self._make_layers(self.in_channel, L, M, B)\n",
    "        self.in_channel *= 2\n",
    "        if is_L_twice:\n",
    "            L *= 2\n",
    "        else:\n",
    "            M *= 2\n",
    "        self.stage3 = self._make_layers(self.in_channel, L, M, B)\n",
    "        self.linear = nn.Linear(self.in_channel * 2, num_classes)\n",
    "        \n",
    "    def _make_layers(self, in_channel, L, M, B):\n",
    "        strides = [2] + [1] * (B - 1)\n",
    "        layers = []\n",
    "        out_channel = self.in_channel\n",
    "        for i, _stride in enumerate(strides):\n",
    "            if i == len(strides) - 1:\n",
    "                out_channel *= 2\n",
    "            layers.append(IGCVBlock(self.in_channel, out_channel, L, M, _stride))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        feat = F.relu(self.bn1(self.conv1(input)))\n",
    "        feat = self.stage1(feat)\n",
    "        feat = self.stage2(feat)\n",
    "        feat = self.stage3(feat)\n",
    "        feat = F.avg_pool2d(feat, 4)\n",
    "        feat = feat.view(feat.size(0), -1)\n",
    "        out  = self.linear(feat)\n",
    "        return out\n",
    "    \n",
    "def IGCV_L24M2(num_classes):\n",
    "    return IGCV_V1(24, 2, 20, False, num_classes)\n",
    "\n",
    "net = IGCV_L24M2(10)\n",
    "x   = Variable(torch.randn(1, 3, 32, 32))\n",
    "y   = net(x)\n",
    "print(y, type(y), y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def conv_init(m):\n",
    "    class_name = m.__class__.__name__\n",
    "    if class_name.find('Conv') != -1:\n",
    "        init.xavier_uniform(m.weight, gain=np.sqrt(2))\n",
    "        init.constant(m.bias, 0)\n",
    "    elif class_name.find('BatchNorm') != -1:\n",
    "        init.constant(m.weight, 1)\n",
    "        init.constant(m.bias, 0)\n",
    "        \n",
    "net = IGCV_L24M2(10)\n",
    "net.apply(conv_init)\n",
    "if is_use_cuda:\n",
    "    net.cuda()\n",
    "    net = nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def lr_schedule(lr, epoch):\n",
    "    optim_factor = 0\n",
    "    if epoch > 160:\n",
    "        optim_factor = 3\n",
    "    elif epoch > 120:\n",
    "        optim_factor = 2\n",
    "    elif epoch > 60:\n",
    "        optim_factor = 1\n",
    "        \n",
    "    return lr * math.pow(0.2, optim_factor)\n",
    "\n",
    "def train(epoch):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct    = 0\n",
    "    total      = 0\n",
    "    optimizer  = optim.SGD(net.parameters(), lr=lr_schedule(lr, epoch), momentum=0.9, weight_decay=5e-4)\n",
    "    \n",
    "    print('Training Epoch: #%d, LR: %.4f'%(epoch, lr_schedule(lr, epoch)))\n",
    "    for idx, (inputs, labels) in enumerate(train_loader):\n",
    "        if is_use_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs        = net(inputs)\n",
    "        loss           = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.data[0]\n",
    "        _, predict = torch.max(outputs.data, 1)\n",
    "        total      += labels.size(0)\n",
    "        correct    += predict.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('[%s] Training Epoch [%d/%d] Iter[%d/%d]\\t\\tLoss: %.4f Acc@1: %.3f'\n",
    "                        % (time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())),\n",
    "                           epoch, num_epochs, idx, len(train_dataset) // batch_size, \n",
    "                          train_loss / (batch_size * (idx + 1)), correct / total))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct   = 0\n",
    "    total     = 0\n",
    "    for idx, (inputs, labels) in enumerate(test_loader):\n",
    "        if is_use_cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs, volatile=True), Variable(labels)\n",
    "        outputs        = net(inputs)\n",
    "        loss           = criterion(outputs, labels)\n",
    "        \n",
    "        test_loss  += loss.data[0]\n",
    "        _, predict = torch.max(outputs.data, 1)\n",
    "        total      += labels.size(0)\n",
    "        correct    += predict.eq(labels.data).cpu().sum()\n",
    "        \n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('[%s] Testing Epoch [%d/%d] Iter[%d/%d]\\t\\tLoss: %.4f Acc@1: %.3f'\n",
    "                        % (time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time())),\n",
    "                           epoch, num_epochs, idx, len(test_dataset) // 80, \n",
    "                          test_loss / (100 * (idx + 1)), correct / total))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    if correct / total > best_acc:\n",
    "        print()\n",
    "        print('Saving Model...')\n",
    "        state = {\n",
    "            'net': net.module if is_use_cuda else net,\n",
    "            'acc': correct / total,\n",
    "            'epoch': epoch\n",
    "        }\n",
    "        if not os.path.isdir('./checkpoint/IGCV_L24M2'):\n",
    "            os.makedirs('./checkpoint/IGCV_L24M2')\n",
    "        torch.save(state, './checkpoint/IGCV_L24M2/IGCV_L24M2_Cifar10.ckpt')\n",
    "        best_acc = correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: #1, LR: 0.1000\n",
      "[2018-06-12 17:45:50] Training Epoch [1/200] Iter[195/195]\t\tLoss: 0.0068 Acc@1: 0.354\n",
      "[2018-06-12 17:45:59] Testing Epoch [1/200] Iter[124/125]\t\tLoss: 0.0146 Acc@1: 0.474\n",
      "Saving Model...\n",
      "\n",
      "Epoch #1 Cost 56s\n",
      "Training Epoch: #2, LR: 0.1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\ProgramData\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:159: UserWarning: Couldn't retrieve source code for container of type IGCV_V1. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "E:\\ProgramData\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:159: UserWarning: Couldn't retrieve source code for container of type IGCVBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "E:\\ProgramData\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:159: UserWarning: Couldn't retrieve source code for container of type ShuffleBlock. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-12 17:46:42] Training Epoch [2/200] Iter[195/195]\t\tLoss: 0.0051 Acc@1: 0.521\n",
      "[2018-06-12 17:46:51] Testing Epoch [2/200] Iter[124/125]\t\tLoss: 0.0158 Acc@1: 0.451\n",
      "Epoch #2 Cost 52s\n",
      "Training Epoch: #3, LR: 0.1000\n",
      "[2018-06-12 17:47:34] Training Epoch [3/200] Iter[195/195]\t\tLoss: 0.0043 Acc@1: 0.608\n",
      "[2018-06-12 17:47:43] Testing Epoch [3/200] Iter[124/125]\t\tLoss: 0.0105 Acc@1: 0.621\n",
      "Saving Model...\n",
      "\n",
      "Epoch #3 Cost 51s\n",
      "Training Epoch: #4, LR: 0.1000\n",
      "[2018-06-12 17:48:26] Training Epoch [4/200] Iter[195/195]\t\tLoss: 0.0037 Acc@1: 0.662\n",
      "[2018-06-12 17:48:34] Testing Epoch [4/200] Iter[124/125]\t\tLoss: 0.0119 Acc@1: 0.601\n",
      "Epoch #4 Cost 51s\n",
      "Training Epoch: #5, LR: 0.1000\n",
      "[2018-06-12 17:49:17] Training Epoch [5/200] Iter[195/195]\t\tLoss: 0.0033 Acc@1: 0.705\n",
      "[2018-06-12 17:49:26] Testing Epoch [5/200] Iter[124/125]\t\tLoss: 0.0093 Acc@1: 0.682\n",
      "Saving Model...\n",
      "\n",
      "Epoch #5 Cost 51s\n",
      "Training Epoch: #6, LR: 0.1000\n",
      "[2018-06-12 17:50:09] Training Epoch [6/200] Iter[195/195]\t\tLoss: 0.0029 Acc@1: 0.737\n",
      "[2018-06-12 17:50:18] Testing Epoch [6/200] Iter[124/125]\t\tLoss: 0.0091 Acc@1: 0.693\n",
      "Saving Model...\n",
      "\n",
      "Epoch #6 Cost 51s\n",
      "Training Epoch: #7, LR: 0.1000\n",
      "[2018-06-12 17:51:01] Training Epoch [7/200] Iter[195/195]\t\tLoss: 0.0027 Acc@1: 0.758\n",
      "[2018-06-12 17:51:10] Testing Epoch [7/200] Iter[124/125]\t\tLoss: 0.0082 Acc@1: 0.712\n",
      "Saving Model...\n",
      "\n",
      "Epoch #7 Cost 52s\n",
      "Training Epoch: #8, LR: 0.1000\n",
      "[2018-06-12 17:51:53] Training Epoch [8/200] Iter[195/195]\t\tLoss: 0.0026 Acc@1: 0.771\n",
      "[2018-06-12 17:52:02] Testing Epoch [8/200] Iter[124/125]\t\tLoss: 0.0084 Acc@1: 0.716\n",
      "Saving Model...\n",
      "\n",
      "Epoch #8 Cost 52s\n",
      "Training Epoch: #9, LR: 0.1000\n",
      "[2018-06-12 17:52:46] Training Epoch [9/200] Iter[195/195]\t\tLoss: 0.0024 Acc@1: 0.785\n",
      "[2018-06-12 17:52:55] Testing Epoch [9/200] Iter[124/125]\t\tLoss: 0.0076 Acc@1: 0.745\n",
      "Saving Model...\n",
      "\n",
      "Epoch #9 Cost 52s\n",
      "Training Epoch: #10, LR: 0.1000\n",
      "[2018-06-12 17:53:39] Training Epoch [10/200] Iter[195/195]\t\tLoss: 0.0023 Acc@1: 0.792\n",
      "[2018-06-12 17:53:48] Testing Epoch [10/200] Iter[124/125]\t\tLoss: 0.0070 Acc@1: 0.763\n",
      "Saving Model...\n",
      "\n",
      "Epoch #10 Cost 52s\n",
      "Training Epoch: #11, LR: 0.1000\n",
      "[2018-06-12 17:54:32] Training Epoch [11/200] Iter[195/195]\t\tLoss: 0.0022 Acc@1: 0.799\n",
      "[2018-06-12 17:54:41] Testing Epoch [11/200] Iter[124/125]\t\tLoss: 0.0084 Acc@1: 0.725\n",
      "Epoch #11 Cost 53s\n",
      "Training Epoch: #12, LR: 0.1000\n",
      "[2018-06-12 17:55:24] Training Epoch [12/200] Iter[195/195]\t\tLoss: 0.0022 Acc@1: 0.803\n",
      "[2018-06-12 17:55:32] Testing Epoch [12/200] Iter[124/125]\t\tLoss: 0.0073 Acc@1: 0.757\n",
      "Epoch #12 Cost 51s\n",
      "Training Epoch: #13, LR: 0.1000\n",
      "[2018-06-12 17:56:16] Training Epoch [13/200] Iter[195/195]\t\tLoss: 0.0021 Acc@1: 0.812\n",
      "[2018-06-12 17:56:25] Testing Epoch [13/200] Iter[124/125]\t\tLoss: 0.0066 Acc@1: 0.781\n",
      "Saving Model...\n",
      "\n",
      "Epoch #13 Cost 52s\n",
      "Training Epoch: #14, LR: 0.1000\n",
      "[2018-06-12 17:57:09] Training Epoch [14/200] Iter[195/195]\t\tLoss: 0.0021 Acc@1: 0.813\n",
      "[2018-06-12 17:57:18] Testing Epoch [14/200] Iter[124/125]\t\tLoss: 0.0067 Acc@1: 0.767\n",
      "Epoch #14 Cost 52s\n",
      "Training Epoch: #15, LR: 0.1000\n",
      "[2018-06-12 17:58:02] Training Epoch [15/200] Iter[195/195]\t\tLoss: 0.0021 Acc@1: 0.817\n",
      "[2018-06-12 17:58:11] Testing Epoch [15/200] Iter[124/125]\t\tLoss: 0.0072 Acc@1: 0.757\n",
      "Epoch #15 Cost 53s\n",
      "Training Epoch: #16, LR: 0.1000\n",
      "[2018-06-12 17:58:55] Training Epoch [16/200] Iter[195/195]\t\tLoss: 0.0021 Acc@1: 0.818\n",
      "[2018-06-12 17:59:04] Testing Epoch [16/200] Iter[124/125]\t\tLoss: 0.0068 Acc@1: 0.774\n",
      "Epoch #16 Cost 52s\n",
      "Training Epoch: #17, LR: 0.1000\n",
      "[2018-06-12 17:59:48] Training Epoch [17/200] Iter[195/195]\t\tLoss: 0.0020 Acc@1: 0.820\n",
      "[2018-06-12 17:59:57] Testing Epoch [17/200] Iter[124/125]\t\tLoss: 0.0061 Acc@1: 0.791\n",
      "Saving Model...\n",
      "\n",
      "Epoch #17 Cost 52s\n",
      "Training Epoch: #18, LR: 0.1000\n",
      "[2018-06-12 18:00:41] Training Epoch [18/200] Iter[195/195]\t\tLoss: 0.0020 Acc@1: 0.821\n",
      "[2018-06-12 18:00:50] Testing Epoch [18/200] Iter[124/125]\t\tLoss: 0.0074 Acc@1: 0.761\n",
      "Epoch #18 Cost 53s\n",
      "Training Epoch: #19, LR: 0.1000\n",
      "[2018-06-12 18:01:34] Training Epoch [19/200] Iter[195/195]\t\tLoss: 0.0020 Acc@1: 0.827\n",
      "[2018-06-12 18:01:43] Testing Epoch [19/200] Iter[124/125]\t\tLoss: 0.0081 Acc@1: 0.740\n",
      "Epoch #19 Cost 52s\n",
      "Training Epoch: #20, LR: 0.1000\n",
      "[2018-06-12 18:02:26] Training Epoch [20/200] Iter[195/195]\t\tLoss: 0.0020 Acc@1: 0.826\n",
      "[2018-06-12 18:02:35] Testing Epoch [20/200] Iter[124/125]\t\tLoss: 0.0068 Acc@1: 0.772\n",
      "Epoch #20 Cost 52s\n",
      "Training Epoch: #21, LR: 0.1000\n",
      "[2018-06-12 18:03:19] Training Epoch [21/200] Iter[195/195]\t\tLoss: 0.0019 Acc@1: 0.829\n",
      "[2018-06-12 18:03:28] Testing Epoch [21/200] Iter[124/125]\t\tLoss: 0.0081 Acc@1: 0.731\n",
      "Epoch #21 Cost 52s\n",
      "Training Epoch: #22, LR: 0.1000\n",
      "[2018-06-12 18:04:11] Training Epoch [22/200] Iter[195/195]\t\tLoss: 0.0019 Acc@1: 0.829\n",
      "[2018-06-12 18:04:20] Testing Epoch [22/200] Iter[124/125]\t\tLoss: 0.0088 Acc@1: 0.715\n",
      "Epoch #22 Cost 51s\n",
      "Training Epoch: #23, LR: 0.1000\n",
      "[2018-06-12 18:05:03] Training Epoch [23/200] Iter[195/195]\t\tLoss: 0.0019 Acc@1: 0.832\n",
      "[2018-06-12 18:05:12] Testing Epoch [23/200] Iter[124/125]\t\tLoss: 0.0062 Acc@1: 0.788\n",
      "Epoch #23 Cost 52s\n",
      "Training Epoch: #24, LR: 0.1000\n",
      "[2018-06-12 18:05:55] Training Epoch [24/200] Iter[195/195]\t\tLoss: 0.0019 Acc@1: 0.832\n",
      "[2018-06-12 18:06:04] Testing Epoch [24/200] Iter[124/125]\t\tLoss: 0.0065 Acc@1: 0.781\n",
      "Epoch #24 Cost 51s\n",
      "Training Epoch: #25, LR: 0.1000\n",
      "[2018-06-12 18:06:46] Training Epoch [25/200] Iter[195/195]\t\tLoss: 0.0019 Acc@1: 0.838\n",
      "[2018-06-12 18:06:55] Testing Epoch [25/200] Iter[124/125]\t\tLoss: 0.0059 Acc@1: 0.804\n",
      "Saving Model...\n",
      "\n",
      "Epoch #25 Cost 50s\n",
      "Training Epoch: #26, LR: 0.1000\n",
      "[2018-06-12 18:07:37] Training Epoch [26/200] Iter[195/195]\t\tLoss: 0.0018 Acc@1: 0.836\n",
      "[2018-06-12 18:07:45] Testing Epoch [26/200] Iter[124/125]\t\tLoss: 0.0081 Acc@1: 0.730\n",
      "Epoch #26 Cost 50s\n",
      "Training Epoch: #27, LR: 0.1000\n",
      "[2018-06-12 18:08:27] Training Epoch [27/200] Iter[195/195]\t\tLoss: 0.0018 Acc@1: 0.837\n",
      "[2018-06-12 18:08:36] Testing Epoch [27/200] Iter[124/125]\t\tLoss: 0.0107 Acc@1: 0.673\n",
      "Epoch #27 Cost 50s\n",
      "Training Epoch: #28, LR: 0.1000\n",
      "[2018-06-12 18:09:18] Training Epoch [28/200] Iter[195/195]\t\tLoss: 0.0018 Acc@1: 0.838\n",
      "[2018-06-12 18:09:27] Testing Epoch [28/200] Iter[124/125]\t\tLoss: 0.0061 Acc@1: 0.795\n",
      "Epoch #28 Cost 50s\n",
      "Training Epoch: #29, LR: 0.1000\n",
      "[2018-06-12 18:10:08] Training Epoch [29/200] Iter[195/195]\t\tLoss: 0.0018 Acc@1: 0.840\n",
      "[2018-06-12 18:10:17] Testing Epoch [29/200] Iter[124/125]\t\tLoss: 0.0068 Acc@1: 0.774\n",
      "Epoch #29 Cost 50s\n",
      "Training Epoch: #30, LR: 0.1000\n",
      "[2018-06-12 18:10:59] Training Epoch [30/200] Iter[195/195]\t\tLoss: 0.0018 Acc@1: 0.841\n",
      "[2018-06-12 18:11:08] Testing Epoch [30/200] Iter[124/125]\t\tLoss: 0.0072 Acc@1: 0.762\n",
      "Epoch #30 Cost 51s\n",
      "Training Epoch: #31, LR: 0.1000\n",
      "[2018-06-12 18:11:50] Training Epoch [31/200] Iter[195/195]\t\tLoss: 0.0018 Acc@1: 0.842\n",
      "[2018-06-12 18:11:59] Testing Epoch [31/200] Iter[124/125]\t\tLoss: 0.0090 Acc@1: 0.709\n",
      "Epoch #31 Cost 50s\n",
      "Training Epoch: #32, LR: 0.1000\n",
      "[2018-06-12 18:12:41] Training Epoch [32/200] Iter[195/195]\t\tLoss: 0.0018 Acc@1: 0.845\n",
      "[2018-06-12 18:12:49] Testing Epoch [32/200] Iter[124/125]\t\tLoss: 0.0066 Acc@1: 0.787\n",
      "Epoch #32 Cost 50s\n",
      "Training Epoch: #33, LR: 0.1000\n",
      "[2018-06-12 18:13:31] Training Epoch [33/200] Iter[195/195]\t\tLoss: 0.0018 Acc@1: 0.846\n",
      "[2018-06-12 18:13:40] Testing Epoch [33/200] Iter[124/125]\t\tLoss: 0.0062 Acc@1: 0.788\n",
      "Epoch #33 Cost 50s\n",
      "Training Epoch: #34, LR: 0.1000\n",
      "[2018-06-12 18:14:22] Training Epoch [34/200] Iter[195/195]\t\tLoss: 0.0017 Acc@1: 0.848\n",
      "[2018-06-12 18:14:31] Testing Epoch [34/200] Iter[124/125]\t\tLoss: 0.0077 Acc@1: 0.745\n",
      "Epoch #34 Cost 50s\n",
      "Training Epoch: #35, LR: 0.1000\n",
      "[2018-06-12 18:15:13] Training Epoch [35/200] Iter[195/195]\t\tLoss: 0.0018 Acc@1: 0.843\n",
      "[2018-06-12 18:15:21] Testing Epoch [35/200] Iter[124/125]\t\tLoss: 0.0068 Acc@1: 0.772\n",
      "Epoch #35 Cost 50s\n",
      "Training Epoch: #36, LR: 0.1000\n",
      "[2018-06-12 18:16:02] Training Epoch [36/200] Iter[195/195]\t\tLoss: 0.0017 Acc@1: 0.847\n",
      "[2018-06-12 18:16:11] Testing Epoch [36/200] Iter[124/125]\t\tLoss: 0.0060 Acc@1: 0.804\n",
      "Epoch #36 Cost 49s\n",
      "Training Epoch: #37, LR: 0.1000\n",
      "[2018-06-12 18:16:52] Training Epoch [37/200] Iter[195/195]\t\tLoss: 0.0017 Acc@1: 0.847\n",
      "[2018-06-12 18:17:01] Testing Epoch [37/200] Iter[124/125]\t\tLoss: 0.0095 Acc@1: 0.711\n",
      "Epoch #37 Cost 50s\n",
      "Training Epoch: #38, LR: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-12 18:17:43] Training Epoch [38/200] Iter[195/195]\t\tLoss: 0.0017 Acc@1: 0.847\n",
      "[2018-06-12 18:17:51] Testing Epoch [38/200] Iter[124/125]\t\tLoss: 0.0069 Acc@1: 0.777\n",
      "Epoch #38 Cost 50s\n",
      "Training Epoch: #39, LR: 0.1000\n",
      "[2018-06-12 18:18:33] Training Epoch [39/200] Iter[195/195]\t\tLoss: 0.0017 Acc@1: 0.849\n",
      "[2018-06-12 18:18:42] Testing Epoch [39/200] Iter[124/125]\t\tLoss: 0.0065 Acc@1: 0.791\n",
      "Epoch #39 Cost 50s\n",
      "Training Epoch: #40, LR: 0.1000\n",
      "[2018-06-12 18:19:24] Training Epoch [40/200] Iter[195/195]\t\tLoss: 0.0017 Acc@1: 0.850\n",
      "[2018-06-12 18:19:33] Testing Epoch [40/200] Iter[124/125]\t\tLoss: 0.0062 Acc@1: 0.789\n",
      "Epoch #40 Cost 50s\n",
      "Training Epoch: #41, LR: 0.1000\n",
      "[2018-06-12 18:20:15] Training Epoch [41/200] Iter[195/195]\t\tLoss: 0.0017 Acc@1: 0.853\n",
      "[2018-06-12 18:20:24] Testing Epoch [41/200] Iter[124/125]\t\tLoss: 0.0068 Acc@1: 0.777\n",
      "Epoch #41 Cost 51s\n",
      "Training Epoch: #42, LR: 0.1000\n",
      "[2018-06-12 18:21:06] Training Epoch [42/200] Iter[195/195]\t\tLoss: 0.0017 Acc@1: 0.850\n",
      "[2018-06-12 18:21:15] Testing Epoch [42/200] Iter[124/125]\t\tLoss: 0.0073 Acc@1: 0.764\n",
      "Epoch #42 Cost 51s\n",
      "Training Epoch: #43, LR: 0.1000\n",
      "[2018-06-12 18:21:56] Training Epoch [43/200] Iter[195/195]\t\tLoss: 0.0017 Acc@1: 0.852\n",
      "[2018-06-12 18:22:05] Testing Epoch [43/200] Iter[124/125]\t\tLoss: 0.0061 Acc@1: 0.800\n",
      "Epoch #43 Cost 50s\n",
      "Training Epoch: #44, LR: 0.1000\n",
      "[2018-06-12 18:22:47] Training Epoch [44/200] Iter[195/195]\t\tLoss: 0.0017 Acc@1: 0.850\n",
      "[2018-06-12 18:22:56] Testing Epoch [44/200] Iter[124/125]\t\tLoss: 0.0056 Acc@1: 0.807\n",
      "Saving Model...\n",
      "\n",
      "Epoch #44 Cost 51s\n",
      "Training Epoch: #45, LR: 0.1000\n",
      "[2018-06-12 18:23:38] Training Epoch [45/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.856\n",
      "[2018-06-12 18:23:47] Testing Epoch [45/200] Iter[124/125]\t\tLoss: 0.0062 Acc@1: 0.794\n",
      "Epoch #45 Cost 51s\n",
      "Training Epoch: #46, LR: 0.1000\n",
      "[2018-06-12 18:24:29] Training Epoch [46/200] Iter[195/195]\t\tLoss: 0.0017 Acc@1: 0.852\n",
      "[2018-06-12 18:24:38] Testing Epoch [46/200] Iter[124/125]\t\tLoss: 0.0074 Acc@1: 0.766\n",
      "Epoch #46 Cost 50s\n",
      "Training Epoch: #47, LR: 0.1000\n",
      "[2018-06-12 18:25:20] Training Epoch [47/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.857\n",
      "[2018-06-12 18:25:28] Testing Epoch [47/200] Iter[124/125]\t\tLoss: 0.0072 Acc@1: 0.773\n",
      "Epoch #47 Cost 50s\n",
      "Training Epoch: #48, LR: 0.1000\n",
      "[2018-06-12 18:26:10] Training Epoch [48/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.855\n",
      "[2018-06-12 18:26:19] Testing Epoch [48/200] Iter[124/125]\t\tLoss: 0.0060 Acc@1: 0.804\n",
      "Epoch #48 Cost 50s\n",
      "Training Epoch: #49, LR: 0.1000\n",
      "[2018-06-12 18:27:00] Training Epoch [49/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.854\n",
      "[2018-06-12 18:27:09] Testing Epoch [49/200] Iter[124/125]\t\tLoss: 0.0067 Acc@1: 0.779\n",
      "Epoch #49 Cost 50s\n",
      "Training Epoch: #50, LR: 0.1000\n",
      "[2018-06-12 18:27:51] Training Epoch [50/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.855\n",
      "[2018-06-12 18:28:00] Testing Epoch [50/200] Iter[124/125]\t\tLoss: 0.0059 Acc@1: 0.809\n",
      "Saving Model...\n",
      "\n",
      "Epoch #50 Cost 51s\n",
      "Training Epoch: #51, LR: 0.1000\n",
      "[2018-06-12 18:28:41] Training Epoch [51/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.858\n",
      "[2018-06-12 18:28:50] Testing Epoch [51/200] Iter[124/125]\t\tLoss: 0.0059 Acc@1: 0.802\n",
      "Epoch #51 Cost 50s\n",
      "Training Epoch: #52, LR: 0.1000\n",
      "[2018-06-12 18:29:32] Training Epoch [52/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.856\n",
      "[2018-06-12 18:29:41] Testing Epoch [52/200] Iter[124/125]\t\tLoss: 0.0064 Acc@1: 0.789\n",
      "Epoch #52 Cost 50s\n",
      "Training Epoch: #53, LR: 0.1000\n",
      "[2018-06-12 18:30:24] Training Epoch [53/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.856\n",
      "[2018-06-12 18:30:32] Testing Epoch [53/200] Iter[124/125]\t\tLoss: 0.0064 Acc@1: 0.791\n",
      "Epoch #53 Cost 51s\n",
      "Training Epoch: #54, LR: 0.1000\n",
      "[2018-06-12 18:31:14] Training Epoch [54/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.857\n",
      "[2018-06-12 18:31:23] Testing Epoch [54/200] Iter[124/125]\t\tLoss: 0.0083 Acc@1: 0.743\n",
      "Epoch #54 Cost 50s\n",
      "Training Epoch: #55, LR: 0.1000\n",
      "[2018-06-12 18:32:04] Training Epoch [55/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.858\n",
      "[2018-06-12 18:32:13] Testing Epoch [55/200] Iter[124/125]\t\tLoss: 0.0063 Acc@1: 0.802\n",
      "Epoch #55 Cost 50s\n",
      "Training Epoch: #56, LR: 0.1000\n",
      "[2018-06-12 18:32:55] Training Epoch [56/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.859\n",
      "[2018-06-12 18:33:04] Testing Epoch [56/200] Iter[124/125]\t\tLoss: 0.0069 Acc@1: 0.778\n",
      "Epoch #56 Cost 50s\n",
      "Training Epoch: #57, LR: 0.1000\n",
      "[2018-06-12 18:33:46] Training Epoch [57/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.859\n",
      "[2018-06-12 18:33:55] Testing Epoch [57/200] Iter[124/125]\t\tLoss: 0.0075 Acc@1: 0.759\n",
      "Epoch #57 Cost 50s\n",
      "Training Epoch: #58, LR: 0.1000\n",
      "[2018-06-12 18:34:37] Training Epoch [58/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.858\n",
      "[2018-06-12 18:34:46] Testing Epoch [58/200] Iter[124/125]\t\tLoss: 0.0057 Acc@1: 0.813\n",
      "Saving Model...\n",
      "\n",
      "Epoch #58 Cost 51s\n",
      "Training Epoch: #59, LR: 0.1000\n",
      "[2018-06-12 18:35:28] Training Epoch [59/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.860\n",
      "[2018-06-12 18:35:37] Testing Epoch [59/200] Iter[124/125]\t\tLoss: 0.0074 Acc@1: 0.762\n",
      "Epoch #59 Cost 50s\n",
      "Training Epoch: #60, LR: 0.1000\n",
      "[2018-06-12 18:36:19] Training Epoch [60/200] Iter[195/195]\t\tLoss: 0.0016 Acc@1: 0.861\n",
      "[2018-06-12 18:36:28] Testing Epoch [60/200] Iter[124/125]\t\tLoss: 0.0051 Acc@1: 0.832\n",
      "Saving Model...\n",
      "\n",
      "Epoch #60 Cost 51s\n",
      "Training Epoch: #61, LR: 0.0200\n",
      "[2018-06-12 18:37:11] Training Epoch [61/200] Iter[195/195]\t\tLoss: 0.0011 Acc@1: 0.901\n",
      "[2018-06-12 18:37:20] Testing Epoch [61/200] Iter[124/125]\t\tLoss: 0.0035 Acc@1: 0.880\n",
      "Saving Model...\n",
      "\n",
      "Epoch #61 Cost 51s\n",
      "Training Epoch: #62, LR: 0.0200\n",
      "[2018-06-12 18:38:02] Training Epoch [62/200] Iter[195/195]\t\tLoss: 0.0010 Acc@1: 0.916\n",
      "[2018-06-12 18:38:11] Testing Epoch [62/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.891\n",
      "Saving Model...\n",
      "\n",
      "Epoch #62 Cost 51s\n",
      "Training Epoch: #63, LR: 0.0200\n",
      "[2018-06-12 18:38:53] Training Epoch [63/200] Iter[195/195]\t\tLoss: 0.0009 Acc@1: 0.921\n",
      "[2018-06-12 18:39:01] Testing Epoch [63/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.890\n",
      "Epoch #63 Cost 50s\n",
      "Training Epoch: #64, LR: 0.0200\n",
      "[2018-06-12 18:39:44] Training Epoch [64/200] Iter[195/195]\t\tLoss: 0.0009 Acc@1: 0.925\n",
      "[2018-06-12 18:39:53] Testing Epoch [64/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.894\n",
      "Saving Model...\n",
      "\n",
      "Epoch #64 Cost 51s\n",
      "Training Epoch: #65, LR: 0.0200\n",
      "[2018-06-12 18:40:34] Training Epoch [65/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.926\n",
      "[2018-06-12 18:40:43] Testing Epoch [65/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.890\n",
      "Epoch #65 Cost 50s\n",
      "Training Epoch: #66, LR: 0.0200\n",
      "[2018-06-12 18:41:25] Training Epoch [66/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.929\n",
      "[2018-06-12 18:41:34] Testing Epoch [66/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.892\n",
      "Epoch #66 Cost 50s\n",
      "Training Epoch: #67, LR: 0.0200\n",
      "[2018-06-12 18:42:16] Training Epoch [67/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.930\n",
      "[2018-06-12 18:42:24] Testing Epoch [67/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.892\n",
      "Epoch #67 Cost 50s\n",
      "Training Epoch: #68, LR: 0.0200\n",
      "[2018-06-12 18:43:07] Training Epoch [68/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.929\n",
      "[2018-06-12 18:43:16] Testing Epoch [68/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.890\n",
      "Epoch #68 Cost 51s\n",
      "Training Epoch: #69, LR: 0.0200\n",
      "[2018-06-12 18:43:58] Training Epoch [69/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.931\n",
      "[2018-06-12 18:44:06] Testing Epoch [69/200] Iter[124/125]\t\tLoss: 0.0035 Acc@1: 0.889\n",
      "Epoch #69 Cost 50s\n",
      "Training Epoch: #70, LR: 0.0200\n",
      "[2018-06-12 18:44:48] Training Epoch [70/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.932\n",
      "[2018-06-12 18:44:57] Testing Epoch [70/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.892\n",
      "Epoch #70 Cost 50s\n",
      "Training Epoch: #71, LR: 0.0200\n",
      "[2018-06-12 18:45:38] Training Epoch [71/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.932\n",
      "[2018-06-12 18:45:47] Testing Epoch [71/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.894\n",
      "Saving Model...\n",
      "\n",
      "Epoch #71 Cost 50s\n",
      "Training Epoch: #72, LR: 0.0200\n",
      "[2018-06-12 18:46:29] Training Epoch [72/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.933\n",
      "[2018-06-12 18:46:38] Testing Epoch [72/200] Iter[124/125]\t\tLoss: 0.0035 Acc@1: 0.891\n",
      "Epoch #72 Cost 50s\n",
      "Training Epoch: #73, LR: 0.0200\n",
      "[2018-06-12 18:47:20] Training Epoch [73/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.932\n",
      "[2018-06-12 18:47:29] Testing Epoch [73/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.888\n",
      "Epoch #73 Cost 50s\n",
      "Training Epoch: #74, LR: 0.0200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-12 18:48:11] Training Epoch [74/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.931\n",
      "[2018-06-12 18:48:20] Testing Epoch [74/200] Iter[124/125]\t\tLoss: 0.0038 Acc@1: 0.879\n",
      "Epoch #74 Cost 51s\n",
      "Training Epoch: #75, LR: 0.0200\n",
      "[2018-06-12 18:49:01] Training Epoch [75/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.935\n",
      "[2018-06-12 18:49:09] Testing Epoch [75/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.889\n",
      "Epoch #75 Cost 49s\n",
      "Training Epoch: #76, LR: 0.0200\n",
      "[2018-06-12 18:49:51] Training Epoch [76/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.933\n",
      "[2018-06-12 18:50:00] Testing Epoch [76/200] Iter[124/125]\t\tLoss: 0.0036 Acc@1: 0.885\n",
      "Epoch #76 Cost 50s\n",
      "Training Epoch: #77, LR: 0.0200\n",
      "[2018-06-12 18:50:41] Training Epoch [77/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.933\n",
      "[2018-06-12 18:50:50] Testing Epoch [77/200] Iter[124/125]\t\tLoss: 0.0037 Acc@1: 0.881\n",
      "Epoch #77 Cost 50s\n",
      "Training Epoch: #78, LR: 0.0200\n",
      "[2018-06-12 18:51:31] Training Epoch [78/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.933\n",
      "[2018-06-12 18:51:40] Testing Epoch [78/200] Iter[124/125]\t\tLoss: 0.0035 Acc@1: 0.891\n",
      "Epoch #78 Cost 50s\n",
      "Training Epoch: #79, LR: 0.0200\n",
      "[2018-06-12 18:52:23] Training Epoch [79/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.934\n",
      "[2018-06-12 18:52:31] Testing Epoch [79/200] Iter[124/125]\t\tLoss: 0.0035 Acc@1: 0.885\n",
      "Epoch #79 Cost 51s\n",
      "Training Epoch: #80, LR: 0.0200\n",
      "[2018-06-12 18:53:14] Training Epoch [80/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.934\n",
      "[2018-06-12 18:53:23] Testing Epoch [80/200] Iter[124/125]\t\tLoss: 0.0036 Acc@1: 0.881\n",
      "Epoch #80 Cost 51s\n",
      "Training Epoch: #81, LR: 0.0200\n",
      "[2018-06-12 18:54:05] Training Epoch [81/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.932\n",
      "[2018-06-12 18:54:14] Testing Epoch [81/200] Iter[124/125]\t\tLoss: 0.0039 Acc@1: 0.877\n",
      "Epoch #81 Cost 50s\n",
      "Training Epoch: #82, LR: 0.0200\n",
      "[2018-06-12 18:54:56] Training Epoch [82/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.933\n",
      "[2018-06-12 18:55:05] Testing Epoch [82/200] Iter[124/125]\t\tLoss: 0.0043 Acc@1: 0.865\n",
      "Epoch #82 Cost 51s\n",
      "Training Epoch: #83, LR: 0.0200\n",
      "[2018-06-12 18:55:47] Training Epoch [83/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.933\n",
      "[2018-06-12 18:55:56] Testing Epoch [83/200] Iter[124/125]\t\tLoss: 0.0039 Acc@1: 0.879\n",
      "Epoch #83 Cost 50s\n",
      "Training Epoch: #84, LR: 0.0200\n",
      "[2018-06-12 18:56:38] Training Epoch [84/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.932\n",
      "[2018-06-12 18:56:46] Testing Epoch [84/200] Iter[124/125]\t\tLoss: 0.0040 Acc@1: 0.874\n",
      "Epoch #84 Cost 50s\n",
      "Training Epoch: #85, LR: 0.0200\n",
      "[2018-06-12 18:57:29] Training Epoch [85/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.933\n",
      "[2018-06-12 18:57:38] Testing Epoch [85/200] Iter[124/125]\t\tLoss: 0.0040 Acc@1: 0.875\n",
      "Epoch #85 Cost 51s\n",
      "Training Epoch: #86, LR: 0.0200\n",
      "[2018-06-12 18:58:20] Training Epoch [86/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.932\n",
      "[2018-06-12 18:58:29] Testing Epoch [86/200] Iter[124/125]\t\tLoss: 0.0040 Acc@1: 0.873\n",
      "Epoch #86 Cost 50s\n",
      "Training Epoch: #87, LR: 0.0200\n",
      "[2018-06-12 18:59:11] Training Epoch [87/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.934\n",
      "[2018-06-12 18:59:19] Testing Epoch [87/200] Iter[124/125]\t\tLoss: 0.0037 Acc@1: 0.881\n",
      "Epoch #87 Cost 50s\n",
      "Training Epoch: #88, LR: 0.0200\n",
      "[2018-06-12 19:00:01] Training Epoch [88/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.932\n",
      "[2018-06-12 19:00:10] Testing Epoch [88/200] Iter[124/125]\t\tLoss: 0.0036 Acc@1: 0.883\n",
      "Epoch #88 Cost 50s\n",
      "Training Epoch: #89, LR: 0.0200\n",
      "[2018-06-12 19:00:52] Training Epoch [89/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.932\n",
      "[2018-06-12 19:01:01] Testing Epoch [89/200] Iter[124/125]\t\tLoss: 0.0035 Acc@1: 0.885\n",
      "Epoch #89 Cost 50s\n",
      "Training Epoch: #90, LR: 0.0200\n",
      "[2018-06-12 19:01:42] Training Epoch [90/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.932\n",
      "[2018-06-12 19:01:51] Testing Epoch [90/200] Iter[124/125]\t\tLoss: 0.0035 Acc@1: 0.888\n",
      "Epoch #90 Cost 50s\n",
      "Training Epoch: #91, LR: 0.0200\n",
      "[2018-06-12 19:02:33] Training Epoch [91/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.934\n",
      "[2018-06-12 19:02:42] Testing Epoch [91/200] Iter[124/125]\t\tLoss: 0.0038 Acc@1: 0.880\n",
      "Epoch #91 Cost 51s\n",
      "Training Epoch: #92, LR: 0.0200\n",
      "[2018-06-12 19:03:24] Training Epoch [92/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.932\n",
      "[2018-06-12 19:03:33] Testing Epoch [92/200] Iter[124/125]\t\tLoss: 0.0036 Acc@1: 0.885\n",
      "Epoch #92 Cost 50s\n",
      "Training Epoch: #93, LR: 0.0200\n",
      "[2018-06-12 19:04:14] Training Epoch [93/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.933\n",
      "[2018-06-12 19:04:23] Testing Epoch [93/200] Iter[124/125]\t\tLoss: 0.0039 Acc@1: 0.874\n",
      "Epoch #93 Cost 50s\n",
      "Training Epoch: #94, LR: 0.0200\n",
      "[2018-06-12 19:05:06] Training Epoch [94/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.931\n",
      "[2018-06-12 19:05:14] Testing Epoch [94/200] Iter[124/125]\t\tLoss: 0.0038 Acc@1: 0.883\n",
      "Epoch #94 Cost 51s\n",
      "Training Epoch: #95, LR: 0.0200\n",
      "[2018-06-12 19:05:56] Training Epoch [95/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.933\n",
      "[2018-06-12 19:06:05] Testing Epoch [95/200] Iter[124/125]\t\tLoss: 0.0043 Acc@1: 0.864\n",
      "Epoch #95 Cost 50s\n",
      "Training Epoch: #96, LR: 0.0200\n",
      "[2018-06-12 19:06:47] Training Epoch [96/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.933\n",
      "[2018-06-12 19:06:56] Testing Epoch [96/200] Iter[124/125]\t\tLoss: 0.0037 Acc@1: 0.881\n",
      "Epoch #96 Cost 50s\n",
      "Training Epoch: #97, LR: 0.0200\n",
      "[2018-06-12 19:07:38] Training Epoch [97/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.932\n",
      "[2018-06-12 19:07:46] Testing Epoch [97/200] Iter[124/125]\t\tLoss: 0.0042 Acc@1: 0.867\n",
      "Epoch #97 Cost 50s\n",
      "Training Epoch: #98, LR: 0.0200\n",
      "[2018-06-12 19:08:28] Training Epoch [98/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.934\n",
      "[2018-06-12 19:08:37] Testing Epoch [98/200] Iter[124/125]\t\tLoss: 0.0039 Acc@1: 0.874\n",
      "Epoch #98 Cost 50s\n",
      "Training Epoch: #99, LR: 0.0200\n",
      "[2018-06-12 19:09:19] Training Epoch [99/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.930\n",
      "[2018-06-12 19:09:28] Testing Epoch [99/200] Iter[124/125]\t\tLoss: 0.0037 Acc@1: 0.878\n",
      "Epoch #99 Cost 51s\n",
      "Training Epoch: #100, LR: 0.0200\n",
      "[2018-06-12 19:10:10] Training Epoch [100/200] Iter[195/195]\t\tLoss: 0.0008 Acc@1: 0.932\n",
      "[2018-06-12 19:10:19] Testing Epoch [100/200] Iter[124/125]\t\tLoss: 0.0037 Acc@1: 0.882\n",
      "Epoch #100 Cost 50s\n",
      "Training Epoch: #101, LR: 0.0200\n",
      "[2018-06-12 19:11:01] Training Epoch [101/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.934\n",
      "[2018-06-12 19:11:10] Testing Epoch [101/200] Iter[124/125]\t\tLoss: 0.0037 Acc@1: 0.877\n",
      "Epoch #101 Cost 51s\n",
      "Training Epoch: #102, LR: 0.0200\n",
      "[2018-06-12 19:11:52] Training Epoch [102/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.932\n",
      "[2018-06-12 19:12:01] Testing Epoch [102/200] Iter[124/125]\t\tLoss: 0.0037 Acc@1: 0.880\n",
      "Epoch #102 Cost 50s\n",
      "Training Epoch: #103, LR: 0.0200\n",
      "[2018-06-12 19:12:42] Training Epoch [103/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.933\n",
      "[2018-06-12 19:12:51] Testing Epoch [103/200] Iter[124/125]\t\tLoss: 0.0038 Acc@1: 0.880\n",
      "Epoch #103 Cost 50s\n",
      "Training Epoch: #104, LR: 0.0200\n",
      "[2018-06-12 19:13:33] Training Epoch [104/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.933\n",
      "[2018-06-12 19:13:42] Testing Epoch [104/200] Iter[124/125]\t\tLoss: 0.0038 Acc@1: 0.880\n",
      "Epoch #104 Cost 50s\n",
      "Training Epoch: #105, LR: 0.0200\n",
      "[2018-06-12 19:14:24] Training Epoch [105/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.935\n",
      "[2018-06-12 19:14:33] Testing Epoch [105/200] Iter[124/125]\t\tLoss: 0.0039 Acc@1: 0.877\n",
      "Epoch #105 Cost 50s\n",
      "Training Epoch: #106, LR: 0.0200\n",
      "[2018-06-12 19:15:15] Training Epoch [106/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.932\n",
      "[2018-06-12 19:15:23] Testing Epoch [106/200] Iter[124/125]\t\tLoss: 0.0036 Acc@1: 0.890\n",
      "Epoch #106 Cost 50s\n",
      "Training Epoch: #107, LR: 0.0200\n",
      "[2018-06-12 19:16:05] Training Epoch [107/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.933\n",
      "[2018-06-12 19:16:14] Testing Epoch [107/200] Iter[124/125]\t\tLoss: 0.0044 Acc@1: 0.866\n",
      "Epoch #107 Cost 50s\n",
      "Training Epoch: #108, LR: 0.0200\n",
      "[2018-06-12 19:16:56] Training Epoch [108/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.933\n",
      "[2018-06-12 19:17:05] Testing Epoch [108/200] Iter[124/125]\t\tLoss: 0.0043 Acc@1: 0.867\n",
      "Epoch #108 Cost 50s\n",
      "Training Epoch: #109, LR: 0.0200\n",
      "[2018-06-12 19:17:47] Training Epoch [109/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.934\n",
      "[2018-06-12 19:17:56] Testing Epoch [109/200] Iter[124/125]\t\tLoss: 0.0040 Acc@1: 0.872\n",
      "Epoch #109 Cost 50s\n",
      "Training Epoch: #110, LR: 0.0200\n",
      "[2018-06-12 19:18:38] Training Epoch [110/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-12 19:18:47] Testing Epoch [110/200] Iter[124/125]\t\tLoss: 0.0037 Acc@1: 0.882\n",
      "Epoch #110 Cost 50s\n",
      "Training Epoch: #111, LR: 0.0200\n",
      "[2018-06-12 19:19:28] Training Epoch [111/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.934\n",
      "[2018-06-12 19:19:37] Testing Epoch [111/200] Iter[124/125]\t\tLoss: 0.0035 Acc@1: 0.887\n",
      "Epoch #111 Cost 50s\n",
      "Training Epoch: #112, LR: 0.0200\n",
      "[2018-06-12 19:20:19] Training Epoch [112/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.932\n",
      "[2018-06-12 19:20:28] Testing Epoch [112/200] Iter[124/125]\t\tLoss: 0.0043 Acc@1: 0.871\n",
      "Epoch #112 Cost 50s\n",
      "Training Epoch: #113, LR: 0.0200\n",
      "[2018-06-12 19:21:10] Training Epoch [113/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.933\n",
      "[2018-06-12 19:21:19] Testing Epoch [113/200] Iter[124/125]\t\tLoss: 0.0043 Acc@1: 0.868\n",
      "Epoch #113 Cost 51s\n",
      "Training Epoch: #114, LR: 0.0200\n",
      "[2018-06-12 19:22:02] Training Epoch [114/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.935\n",
      "[2018-06-12 19:22:10] Testing Epoch [114/200] Iter[124/125]\t\tLoss: 0.0049 Acc@1: 0.846\n",
      "Epoch #114 Cost 51s\n",
      "Training Epoch: #115, LR: 0.0200\n",
      "[2018-06-12 19:22:52] Training Epoch [115/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.934\n",
      "[2018-06-12 19:23:01] Testing Epoch [115/200] Iter[124/125]\t\tLoss: 0.0043 Acc@1: 0.861\n",
      "Epoch #115 Cost 50s\n",
      "Training Epoch: #116, LR: 0.0200\n",
      "[2018-06-12 19:23:43] Training Epoch [116/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.934\n",
      "[2018-06-12 19:23:52] Testing Epoch [116/200] Iter[124/125]\t\tLoss: 0.0044 Acc@1: 0.868\n",
      "Epoch #116 Cost 51s\n",
      "Training Epoch: #117, LR: 0.0200\n",
      "[2018-06-12 19:24:34] Training Epoch [117/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.936\n",
      "[2018-06-12 19:24:43] Testing Epoch [117/200] Iter[124/125]\t\tLoss: 0.0048 Acc@1: 0.853\n",
      "Epoch #117 Cost 50s\n",
      "Training Epoch: #118, LR: 0.0200\n",
      "[2018-06-12 19:25:25] Training Epoch [118/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.933\n",
      "[2018-06-12 19:25:34] Testing Epoch [118/200] Iter[124/125]\t\tLoss: 0.0043 Acc@1: 0.868\n",
      "Epoch #118 Cost 50s\n",
      "Training Epoch: #119, LR: 0.0200\n",
      "[2018-06-12 19:26:14] Training Epoch [119/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.936\n",
      "[2018-06-12 19:26:23] Testing Epoch [119/200] Iter[124/125]\t\tLoss: 0.0040 Acc@1: 0.879\n",
      "Epoch #119 Cost 49s\n",
      "Training Epoch: #120, LR: 0.0200\n",
      "[2018-06-12 19:27:06] Training Epoch [120/200] Iter[195/195]\t\tLoss: 0.0007 Acc@1: 0.938\n",
      "[2018-06-12 19:27:15] Testing Epoch [120/200] Iter[124/125]\t\tLoss: 0.0037 Acc@1: 0.885\n",
      "Epoch #120 Cost 51s\n",
      "Training Epoch: #121, LR: 0.0040\n",
      "[2018-06-12 19:27:57] Training Epoch [121/200] Iter[195/195]\t\tLoss: 0.0005 Acc@1: 0.957\n",
      "[2018-06-12 19:28:06] Testing Epoch [121/200] Iter[124/125]\t\tLoss: 0.0028 Acc@1: 0.908\n",
      "Saving Model...\n",
      "\n",
      "Epoch #121 Cost 51s\n",
      "Training Epoch: #122, LR: 0.0040\n",
      "[2018-06-12 19:28:48] Training Epoch [122/200] Iter[195/195]\t\tLoss: 0.0004 Acc@1: 0.966\n",
      "[2018-06-12 19:28:56] Testing Epoch [122/200] Iter[124/125]\t\tLoss: 0.0028 Acc@1: 0.911\n",
      "Saving Model...\n",
      "\n",
      "Epoch #122 Cost 50s\n",
      "Training Epoch: #123, LR: 0.0040\n",
      "[2018-06-12 19:29:38] Training Epoch [123/200] Iter[195/195]\t\tLoss: 0.0004 Acc@1: 0.970\n",
      "[2018-06-12 19:29:47] Testing Epoch [123/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.912\n",
      "Saving Model...\n",
      "\n",
      "Epoch #123 Cost 50s\n",
      "Training Epoch: #124, LR: 0.0040\n",
      "[2018-06-12 19:30:29] Training Epoch [124/200] Iter[195/195]\t\tLoss: 0.0003 Acc@1: 0.971\n",
      "[2018-06-12 19:30:38] Testing Epoch [124/200] Iter[124/125]\t\tLoss: 0.0028 Acc@1: 0.913\n",
      "Saving Model...\n",
      "\n",
      "Epoch #124 Cost 50s\n",
      "Training Epoch: #125, LR: 0.0040\n",
      "[2018-06-12 19:31:20] Training Epoch [125/200] Iter[195/195]\t\tLoss: 0.0003 Acc@1: 0.973\n",
      "[2018-06-12 19:31:28] Testing Epoch [125/200] Iter[124/125]\t\tLoss: 0.0028 Acc@1: 0.913\n",
      "Saving Model...\n",
      "\n",
      "Epoch #125 Cost 50s\n",
      "Training Epoch: #126, LR: 0.0040\n",
      "[2018-06-12 19:32:10] Training Epoch [126/200] Iter[195/195]\t\tLoss: 0.0003 Acc@1: 0.974\n",
      "[2018-06-12 19:32:19] Testing Epoch [126/200] Iter[124/125]\t\tLoss: 0.0028 Acc@1: 0.914\n",
      "Saving Model...\n",
      "\n",
      "Epoch #126 Cost 50s\n",
      "Training Epoch: #127, LR: 0.0040\n",
      "[2018-06-12 19:33:01] Training Epoch [127/200] Iter[195/195]\t\tLoss: 0.0003 Acc@1: 0.975\n",
      "[2018-06-12 19:33:09] Testing Epoch [127/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.909\n",
      "Epoch #127 Cost 50s\n",
      "Training Epoch: #128, LR: 0.0040\n",
      "[2018-06-12 19:33:52] Training Epoch [128/200] Iter[195/195]\t\tLoss: 0.0003 Acc@1: 0.977\n",
      "[2018-06-12 19:34:01] Testing Epoch [128/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.915\n",
      "Saving Model...\n",
      "\n",
      "Epoch #128 Cost 51s\n",
      "Training Epoch: #129, LR: 0.0040\n",
      "[2018-06-12 19:34:43] Training Epoch [129/200] Iter[195/195]\t\tLoss: 0.0003 Acc@1: 0.976\n",
      "[2018-06-12 19:34:52] Testing Epoch [129/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.912\n",
      "Epoch #129 Cost 51s\n",
      "Training Epoch: #130, LR: 0.0040\n",
      "[2018-06-12 19:35:34] Training Epoch [130/200] Iter[195/195]\t\tLoss: 0.0003 Acc@1: 0.978\n",
      "[2018-06-12 19:35:42] Testing Epoch [130/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.912\n",
      "Epoch #130 Cost 50s\n",
      "Training Epoch: #131, LR: 0.0040\n",
      "[2018-06-12 19:36:25] Training Epoch [131/200] Iter[195/195]\t\tLoss: 0.0003 Acc@1: 0.978\n",
      "[2018-06-12 19:36:34] Testing Epoch [131/200] Iter[124/125]\t\tLoss: 0.0030 Acc@1: 0.912\n",
      "Epoch #131 Cost 51s\n",
      "Training Epoch: #132, LR: 0.0040\n",
      "[2018-06-12 19:37:16] Training Epoch [132/200] Iter[195/195]\t\tLoss: 0.0003 Acc@1: 0.978\n",
      "[2018-06-12 19:37:24] Testing Epoch [132/200] Iter[124/125]\t\tLoss: 0.0029 Acc@1: 0.913\n",
      "Epoch #132 Cost 50s\n",
      "Training Epoch: #133, LR: 0.0040\n",
      "[2018-06-12 19:38:06] Training Epoch [133/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.980\n",
      "[2018-06-12 19:38:15] Testing Epoch [133/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.911\n",
      "Epoch #133 Cost 50s\n",
      "Training Epoch: #134, LR: 0.0040\n",
      "[2018-06-12 19:38:58] Training Epoch [134/200] Iter[195/195]\t\tLoss: 0.0003 Acc@1: 0.979\n",
      "[2018-06-12 19:39:06] Testing Epoch [134/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.911\n",
      "Epoch #134 Cost 51s\n",
      "Training Epoch: #135, LR: 0.0040\n",
      "[2018-06-12 19:39:48] Training Epoch [135/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.980\n",
      "[2018-06-12 19:39:56] Testing Epoch [135/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.908\n",
      "Epoch #135 Cost 50s\n",
      "Training Epoch: #136, LR: 0.0040\n",
      "[2018-06-12 19:40:38] Training Epoch [136/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.980\n",
      "[2018-06-12 19:40:47] Testing Epoch [136/200] Iter[124/125]\t\tLoss: 0.0031 Acc@1: 0.910\n",
      "Epoch #136 Cost 50s\n",
      "Training Epoch: #137, LR: 0.0040\n",
      "[2018-06-12 19:41:28] Training Epoch [137/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.982\n",
      "[2018-06-12 19:41:37] Testing Epoch [137/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.910\n",
      "Epoch #137 Cost 50s\n",
      "Training Epoch: #138, LR: 0.0040\n",
      "[2018-06-12 19:42:20] Training Epoch [138/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.981\n",
      "[2018-06-12 19:42:28] Testing Epoch [138/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.910\n",
      "Epoch #138 Cost 51s\n",
      "Training Epoch: #139, LR: 0.0040\n",
      "[2018-06-12 19:43:11] Training Epoch [139/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.981\n",
      "[2018-06-12 19:43:20] Testing Epoch [139/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.911\n",
      "Epoch #139 Cost 51s\n",
      "Training Epoch: #140, LR: 0.0040\n",
      "[2018-06-12 19:44:02] Training Epoch [140/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.982\n",
      "[2018-06-12 19:44:11] Testing Epoch [140/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.911\n",
      "Epoch #140 Cost 51s\n",
      "Training Epoch: #141, LR: 0.0040\n",
      "[2018-06-12 19:44:52] Training Epoch [141/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.983\n",
      "[2018-06-12 19:45:01] Testing Epoch [141/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.911\n",
      "Epoch #141 Cost 50s\n",
      "Training Epoch: #142, LR: 0.0040\n",
      "[2018-06-12 19:45:43] Training Epoch [142/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.982\n",
      "[2018-06-12 19:45:52] Testing Epoch [142/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.907\n",
      "Epoch #142 Cost 50s\n",
      "Training Epoch: #143, LR: 0.0040\n",
      "[2018-06-12 19:46:33] Training Epoch [143/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.982\n",
      "[2018-06-12 19:46:42] Testing Epoch [143/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.908\n",
      "Epoch #143 Cost 50s\n",
      "Training Epoch: #144, LR: 0.0040\n",
      "[2018-06-12 19:47:25] Training Epoch [144/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.981\n",
      "[2018-06-12 19:47:33] Testing Epoch [144/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.912\n",
      "Epoch #144 Cost 50s\n",
      "Training Epoch: #145, LR: 0.0040\n",
      "[2018-06-12 19:48:16] Training Epoch [145/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.983\n",
      "[2018-06-12 19:48:24] Testing Epoch [145/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.906\n",
      "Epoch #145 Cost 51s\n",
      "Training Epoch: #146, LR: 0.0040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-12 19:49:06] Training Epoch [146/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.983\n",
      "[2018-06-12 19:49:15] Testing Epoch [146/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.909\n",
      "Epoch #146 Cost 50s\n",
      "Training Epoch: #147, LR: 0.0040\n",
      "[2018-06-12 19:49:57] Training Epoch [147/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.983\n",
      "[2018-06-12 19:50:05] Testing Epoch [147/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.907\n",
      "Epoch #147 Cost 50s\n",
      "Training Epoch: #148, LR: 0.0040\n",
      "[2018-06-12 19:50:48] Training Epoch [148/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.984\n",
      "[2018-06-12 19:50:57] Testing Epoch [148/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.910\n",
      "Epoch #148 Cost 51s\n",
      "Training Epoch: #149, LR: 0.0040\n",
      "[2018-06-12 19:51:38] Training Epoch [149/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.984\n",
      "[2018-06-12 19:51:47] Testing Epoch [149/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.911\n",
      "Epoch #149 Cost 50s\n",
      "Training Epoch: #150, LR: 0.0040\n",
      "[2018-06-12 19:52:29] Training Epoch [150/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.983\n",
      "[2018-06-12 19:52:38] Testing Epoch [150/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.911\n",
      "Epoch #150 Cost 51s\n",
      "Training Epoch: #151, LR: 0.0040\n",
      "[2018-06-12 19:53:20] Training Epoch [151/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.983\n",
      "[2018-06-12 19:53:29] Testing Epoch [151/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.908\n",
      "Epoch #151 Cost 50s\n",
      "Training Epoch: #152, LR: 0.0040\n",
      "[2018-06-12 19:54:11] Training Epoch [152/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.983\n",
      "[2018-06-12 19:54:20] Testing Epoch [152/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.912\n",
      "Epoch #152 Cost 50s\n",
      "Training Epoch: #153, LR: 0.0040\n",
      "[2018-06-12 19:55:02] Training Epoch [153/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.983\n",
      "[2018-06-12 19:55:11] Testing Epoch [153/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.912\n",
      "Epoch #153 Cost 50s\n",
      "Training Epoch: #154, LR: 0.0040\n",
      "[2018-06-12 19:55:53] Training Epoch [154/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.985\n",
      "[2018-06-12 19:56:01] Testing Epoch [154/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.910\n",
      "Epoch #154 Cost 50s\n",
      "Training Epoch: #155, LR: 0.0040\n",
      "[2018-06-12 19:56:44] Training Epoch [155/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.984\n",
      "[2018-06-12 19:56:53] Testing Epoch [155/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.908\n",
      "Epoch #155 Cost 51s\n",
      "Training Epoch: #156, LR: 0.0040\n",
      "[2018-06-12 19:57:35] Training Epoch [156/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.984\n",
      "[2018-06-12 19:57:44] Testing Epoch [156/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.913\n",
      "Epoch #156 Cost 51s\n",
      "Training Epoch: #157, LR: 0.0040\n",
      "[2018-06-12 19:58:26] Training Epoch [157/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.984\n",
      "[2018-06-12 19:58:35] Testing Epoch [157/200] Iter[124/125]\t\tLoss: 0.0035 Acc@1: 0.911\n",
      "Epoch #157 Cost 50s\n",
      "Training Epoch: #158, LR: 0.0040\n",
      "[2018-06-12 19:59:16] Training Epoch [158/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.985\n",
      "[2018-06-12 19:59:25] Testing Epoch [158/200] Iter[124/125]\t\tLoss: 0.0035 Acc@1: 0.910\n",
      "Epoch #158 Cost 50s\n",
      "Training Epoch: #159, LR: 0.0040\n",
      "[2018-06-12 20:00:07] Training Epoch [159/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.985\n",
      "[2018-06-12 20:00:16] Testing Epoch [159/200] Iter[124/125]\t\tLoss: 0.0035 Acc@1: 0.908\n",
      "Epoch #159 Cost 50s\n",
      "Training Epoch: #160, LR: 0.0040\n",
      "[2018-06-12 20:00:58] Training Epoch [160/200] Iter[195/195]\t\tLoss: 0.0002 Acc@1: 0.983\n",
      "[2018-06-12 20:01:06] Testing Epoch [160/200] Iter[124/125]\t\tLoss: 0.0036 Acc@1: 0.906\n",
      "Epoch #160 Cost 50s\n",
      "Training Epoch: #161, LR: 0.0008\n",
      "[2018-06-12 20:01:49] Training Epoch [161/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.989\n",
      "[2018-06-12 20:01:58] Testing Epoch [161/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.914\n",
      "Epoch #161 Cost 51s\n",
      "Training Epoch: #162, LR: 0.0008\n",
      "[2018-06-12 20:02:40] Training Epoch [162/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.989\n",
      "[2018-06-12 20:02:49] Testing Epoch [162/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.915\n",
      "Saving Model...\n",
      "\n",
      "Epoch #162 Cost 50s\n",
      "Training Epoch: #163, LR: 0.0008\n",
      "[2018-06-12 20:03:31] Training Epoch [163/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.990\n",
      "[2018-06-12 20:03:40] Testing Epoch [163/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.914\n",
      "Epoch #163 Cost 50s\n",
      "Training Epoch: #164, LR: 0.0008\n",
      "[2018-06-12 20:04:21] Training Epoch [164/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.992\n",
      "[2018-06-12 20:04:30] Testing Epoch [164/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.914\n",
      "Epoch #164 Cost 50s\n",
      "Training Epoch: #165, LR: 0.0008\n",
      "[2018-06-12 20:05:13] Training Epoch [165/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.992\n",
      "[2018-06-12 20:05:21] Testing Epoch [165/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.913\n",
      "Epoch #165 Cost 51s\n",
      "Training Epoch: #166, LR: 0.0008\n",
      "[2018-06-12 20:06:04] Training Epoch [166/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.992\n",
      "[2018-06-12 20:06:13] Testing Epoch [166/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.914\n",
      "Epoch #166 Cost 51s\n",
      "Training Epoch: #167, LR: 0.0008\n",
      "[2018-06-12 20:06:55] Training Epoch [167/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.992\n",
      "[2018-06-12 20:07:04] Testing Epoch [167/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.915\n",
      "Epoch #167 Cost 50s\n",
      "Training Epoch: #168, LR: 0.0008\n",
      "[2018-06-12 20:07:45] Training Epoch [168/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.992\n",
      "[2018-06-12 20:07:54] Testing Epoch [168/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.915\n",
      "Epoch #168 Cost 50s\n",
      "Training Epoch: #169, LR: 0.0008\n",
      "[2018-06-12 20:08:36] Training Epoch [169/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.993\n",
      "[2018-06-12 20:08:45] Testing Epoch [169/200] Iter[124/125]\t\tLoss: 0.0032 Acc@1: 0.915\n",
      "Saving Model...\n",
      "\n",
      "Epoch #169 Cost 50s\n",
      "Training Epoch: #170, LR: 0.0008\n",
      "[2018-06-12 20:09:27] Training Epoch [170/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.992\n",
      "[2018-06-12 20:09:35] Testing Epoch [170/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.914\n",
      "Epoch #170 Cost 50s\n",
      "Training Epoch: #171, LR: 0.0008\n",
      "[2018-06-12 20:10:18] Training Epoch [171/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.992\n",
      "[2018-06-12 20:10:26] Testing Epoch [171/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.916\n",
      "Saving Model...\n",
      "\n",
      "Epoch #171 Cost 51s\n",
      "Training Epoch: #172, LR: 0.0008\n",
      "[2018-06-12 20:11:09] Training Epoch [172/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.993\n",
      "[2018-06-12 20:11:18] Testing Epoch [172/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.914\n",
      "Epoch #172 Cost 51s\n",
      "Training Epoch: #173, LR: 0.0008\n",
      "[2018-06-12 20:12:00] Training Epoch [173/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.993\n",
      "[2018-06-12 20:12:08] Testing Epoch [173/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.915\n",
      "Epoch #173 Cost 50s\n",
      "Training Epoch: #174, LR: 0.0008\n",
      "[2018-06-12 20:12:50] Training Epoch [174/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.993\n",
      "[2018-06-12 20:12:59] Testing Epoch [174/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.914\n",
      "Epoch #174 Cost 50s\n",
      "Training Epoch: #175, LR: 0.0008\n",
      "[2018-06-12 20:13:41] Training Epoch [175/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:13:50] Testing Epoch [175/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.914\n",
      "Epoch #175 Cost 51s\n",
      "Training Epoch: #176, LR: 0.0008\n",
      "[2018-06-12 20:14:32] Training Epoch [176/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.993\n",
      "[2018-06-12 20:14:41] Testing Epoch [176/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.914\n",
      "Epoch #176 Cost 50s\n",
      "Training Epoch: #177, LR: 0.0008\n",
      "[2018-06-12 20:15:23] Training Epoch [177/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.993\n",
      "[2018-06-12 20:15:31] Testing Epoch [177/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.914\n",
      "Epoch #177 Cost 50s\n",
      "Training Epoch: #178, LR: 0.0008\n",
      "[2018-06-12 20:16:14] Training Epoch [178/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:16:23] Testing Epoch [178/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.914\n",
      "Epoch #178 Cost 51s\n",
      "Training Epoch: #179, LR: 0.0008\n",
      "[2018-06-12 20:17:05] Training Epoch [179/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:17:14] Testing Epoch [179/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.915\n",
      "Epoch #179 Cost 50s\n",
      "Training Epoch: #180, LR: 0.0008\n",
      "[2018-06-12 20:17:56] Training Epoch [180/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.993\n",
      "[2018-06-12 20:18:05] Testing Epoch [180/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.914\n",
      "Epoch #180 Cost 51s\n",
      "Training Epoch: #181, LR: 0.0008\n",
      "[2018-06-12 20:18:47] Training Epoch [181/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-06-12 20:18:55] Testing Epoch [181/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.916\n",
      "Saving Model...\n",
      "\n",
      "Epoch #181 Cost 50s\n",
      "Training Epoch: #182, LR: 0.0008\n",
      "[2018-06-12 20:19:38] Training Epoch [182/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:19:46] Testing Epoch [182/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.914\n",
      "Epoch #182 Cost 51s\n",
      "Training Epoch: #183, LR: 0.0008\n",
      "[2018-06-12 20:20:29] Training Epoch [183/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:20:38] Testing Epoch [183/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.915\n",
      "Epoch #183 Cost 51s\n",
      "Training Epoch: #184, LR: 0.0008\n",
      "[2018-06-12 20:21:20] Training Epoch [184/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:21:29] Testing Epoch [184/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.915\n",
      "Epoch #184 Cost 51s\n",
      "Training Epoch: #185, LR: 0.0008\n",
      "[2018-06-12 20:22:10] Training Epoch [185/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:22:19] Testing Epoch [185/200] Iter[124/125]\t\tLoss: 0.0033 Acc@1: 0.916\n",
      "Epoch #185 Cost 50s\n",
      "Training Epoch: #186, LR: 0.0008\n",
      "[2018-06-12 20:23:02] Training Epoch [186/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:23:10] Testing Epoch [186/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.914\n",
      "Epoch #186 Cost 51s\n",
      "Training Epoch: #187, LR: 0.0008\n",
      "[2018-06-12 20:23:53] Training Epoch [187/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:24:02] Testing Epoch [187/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.916\n",
      "Epoch #187 Cost 51s\n",
      "Training Epoch: #188, LR: 0.0008\n",
      "[2018-06-12 20:24:44] Training Epoch [188/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:24:53] Testing Epoch [188/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.915\n",
      "Epoch #188 Cost 51s\n",
      "Training Epoch: #189, LR: 0.0008\n",
      "[2018-06-12 20:25:36] Training Epoch [189/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.995\n",
      "[2018-06-12 20:25:44] Testing Epoch [189/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.914\n",
      "Epoch #189 Cost 51s\n",
      "Training Epoch: #190, LR: 0.0008\n",
      "[2018-06-12 20:26:27] Training Epoch [190/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:26:36] Testing Epoch [190/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.918\n",
      "Saving Model...\n",
      "\n",
      "Epoch #190 Cost 51s\n",
      "Training Epoch: #191, LR: 0.0008\n",
      "[2018-06-12 20:27:18] Training Epoch [191/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.993\n",
      "[2018-06-12 20:27:26] Testing Epoch [191/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.916\n",
      "Epoch #191 Cost 50s\n",
      "Training Epoch: #192, LR: 0.0008\n",
      "[2018-06-12 20:28:09] Training Epoch [192/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:28:18] Testing Epoch [192/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.915\n",
      "Epoch #192 Cost 51s\n",
      "Training Epoch: #193, LR: 0.0008\n",
      "[2018-06-12 20:29:00] Training Epoch [193/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:29:09] Testing Epoch [193/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.915\n",
      "Epoch #193 Cost 51s\n",
      "Training Epoch: #194, LR: 0.0008\n",
      "[2018-06-12 20:29:51] Training Epoch [194/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.995\n",
      "[2018-06-12 20:30:00] Testing Epoch [194/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.916\n",
      "Epoch #194 Cost 51s\n",
      "Training Epoch: #195, LR: 0.0008\n",
      "[2018-06-12 20:30:43] Training Epoch [195/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:30:52] Testing Epoch [195/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.916\n",
      "Epoch #195 Cost 51s\n",
      "Training Epoch: #196, LR: 0.0008\n",
      "[2018-06-12 20:31:35] Training Epoch [196/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:31:44] Testing Epoch [196/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.916\n",
      "Epoch #196 Cost 51s\n",
      "Training Epoch: #197, LR: 0.0008\n",
      "[2018-06-12 20:32:27] Training Epoch [197/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:32:36] Testing Epoch [197/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.915\n",
      "Epoch #197 Cost 52s\n",
      "Training Epoch: #198, LR: 0.0008\n",
      "[2018-06-12 20:33:18] Training Epoch [198/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:33:27] Testing Epoch [198/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.915\n",
      "Epoch #198 Cost 51s\n",
      "Training Epoch: #199, LR: 0.0008\n",
      "[2018-06-12 20:34:10] Training Epoch [199/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.994\n",
      "[2018-06-12 20:34:19] Testing Epoch [199/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.916\n",
      "Epoch #199 Cost 51s\n",
      "Training Epoch: #200, LR: 0.0008\n",
      "[2018-06-12 20:35:01] Training Epoch [200/200] Iter[195/195]\t\tLoss: 0.0001 Acc@1: 0.995\n",
      "[2018-06-12 20:35:10] Testing Epoch [200/200] Iter[124/125]\t\tLoss: 0.0034 Acc@1: 0.915\n",
      "Epoch #200 Cost 50s\n",
      "Best Acc@1: 91.7800\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for _epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "    start_time = time.time()\n",
    "    train(_epoch)\n",
    "    print()\n",
    "    test(_epoch)\n",
    "    print()\n",
    "    end_time   = time.time()\n",
    "    print('Epoch #%d Cost %ds' % (_epoch, end_time - start_time))\n",
    "    \n",
    "print('Best Acc@1: %.4f' % (best_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
